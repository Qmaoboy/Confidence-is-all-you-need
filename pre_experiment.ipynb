{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama is an American politician and attorney who served as the 44th president of the United States from 2009 to 2017. He was the first African American to hold the office. Before his presidency, he served as a U.S. senator from Illinois from 2005 to 2008 and as an Illinois state senator from 1997 to 2004. Obama is a member of the Democratic Party.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"answer\": \"Barack Obama is an American politician and attorney who served as the 44th president of the United States from 2009 to 2017. He was the first African American to hold the office. Before his presidency, he served as a U.S. senator from Illinois from 2005 to 2008 and as an Illinois state senator from 1997 to 2004. Obama is a member of the Democratic Party.\",\n",
      "  \"confidence_score\": 0.99\n",
      "}\n",
      "```\n",
      "**************************************************\n",
      "{'Answer': 'Barack Obama is an American politician and attorney who served as the 44th president of the United States from 2009 to 2017. He was the first African American to hold the office. Before his presidency, he served as a U.S. senator from Illinois from 2005 to 2008 and as an Illinois state senator from 1997 to 2004. Obama is a member of the Democratic Party.', 'Confidence': 0.99}\n",
      "**************************************************\n",
      "{'Answer': 'Barack Obama is an American politician and attorney who served as the 44th president of the United States from 2009 to 2017. He was the first African American to hold the office. Before his presidency, he served as a U.S. senator from Illinois from 2005 to 2008 and as an Illinois state senator from 1997 to 2004. Obama is a member of the Democratic Party. One of his key accomplishments during his presidency was the implementation of the Affordable Care Act.', 'Confidence': 0.99}\n"
     ]
    }
   ],
   "source": [
    "import textgrad as tg\n",
    "import os,json,re,yaml\n",
    "\n",
    "if os.path.isfile(\"base_work/api_key.yml\"):\n",
    "    with open(\"base_work/api_key.yml\",\"r\") as f:\n",
    "        key=yaml.safe_load(f)\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = key['openai']['api_key']\n",
    "def parser(text):\n",
    "    answer_match = re.search(r'\"answer\": \"(.*?)\"', text)\n",
    "    if answer_match:\n",
    "        answer = answer_match.group(1)\n",
    "    # Regex to capture the confidence score\n",
    "    confidence_match = re.search(r'\"confidence_score\": (\\d+\\.\\d+)', text)\n",
    "    if confidence_match:\n",
    "        confidence_score = float(confidence_match.group(1))\n",
    "\n",
    "    return {\"Answer\":answer,\"Confidence\":confidence_score}\n",
    "\n",
    "tg.set_backward_engine(\"gpt-3.5-turbo-0125\", override=True)\n",
    "\n",
    "# Step 1: Get an initial response from an LLM.\n",
    "model = tg.BlackboxLLM(\"gpt-4o\")\n",
    "question_string = (\"who is brack obama?\"\n",
    "                   \"provide confidence score to the answer in json\")\n",
    "\n",
    "question = tg.Variable(question_string,\n",
    "                       role_description=\"question to the LLM\",\n",
    "                       requires_grad=False)\n",
    "\n",
    "answer = model(question)\n",
    "print(answer)\n",
    "result=parser(str(answer))\n",
    "print(\"*\"*50)\n",
    "print(result)\n",
    "answer.set_role_description(\"concise and accurate answer to the question\")\n",
    "\n",
    "# Step 2: Define the loss function and the optimizer, just like in PyTorch!\n",
    "# Here, we don't have SGD, but we have TGD (Textual Gradient Descent)\n",
    "# that works with \"textual gradients\".\n",
    "optimizer = tg.TGD(parameters=[answer])\n",
    "evaluation_instruction = (f\"Here's a question: {question_string}. \"\n",
    "                           \"Evaluate any given answer to this question, \"\n",
    "                           \"be smart, logical, and very critical. \"\n",
    "                           \"Just provide concise feedback.\"\n",
    "                           )\n",
    "\n",
    "\n",
    "# TextLoss is a natural-language specified loss function that describes\n",
    "# how we want to evaluate the reasoning.\n",
    "loss_fn = tg.TextLoss(evaluation_instruction)\n",
    "\n",
    "# Step 3: Do the loss computation, backward pass, and update the punchline.\n",
    "# Exact same syntax as PyTorch!\n",
    "loss = loss_fn(answer)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(\"*\"*50)\n",
    "result['Answer']=str(answer)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4-turbo Data size 50\n",
      "gpt-3.5-turbo-0125 Data size 49\n",
      "claude-3-5-sonnet-20240620 Data size 48\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from random import randint\n",
    "class demo:\n",
    "    def __init__(self,path) -> None:\n",
    "        with open(path,\"r\") as f:\n",
    "            self.refine_data=json.load(f)\n",
    "\n",
    "    def Get_original_result(self,Question):\n",
    "        with open('CAPR/response_result/20240601/din0s_asqa_gpt-3.5-turbo-0125_vanilla_Long_QA.json','r') as af:\n",
    "            self.origin_data=json.load(af)\n",
    "\n",
    "        print(self.origin_data[0].keys())\n",
    "\n",
    "        for i in self.origin_data:\n",
    "            if i[\"Question\"]==Question:\n",
    "                self.result['origin_answer']=i[\"Answer\"]\n",
    "                self.result['origin_confidence']=i[\"Confidence\"]\n",
    "                self.result['origin_Instruction']=i['Prompt'][\"Instruction\"]\n",
    "\n",
    "    def Refine_exmple(self,api_model):\n",
    "        temp_acc=0.1\n",
    "        self.result={}\n",
    "        data_size=len(self.refine_data[api_model]['Evaluate_result']['Accuracy'])\n",
    "        print(f\"{api_model} Data size {data_size}\")\n",
    "        idx=randint(0,data_size-1)\n",
    "        for idx in range(data_size):\n",
    "            if self.refine_data[api_model]['Example']['old_prompt'][idx]['Question']==\"Question : When was the first apple i phone made?\":\n",
    "                # print(self.refine_data[api_model]['Example'].keys())\n",
    "                self.result[\"Question\"]=self.refine_data[api_model]['Example']['old_prompt'][idx]['Question']\n",
    "                self.result[\"Origin_Instruction\"]=self.refine_data[api_model]['Example']['old_prompt'][idx]['Instruction']\n",
    "                self.result[\"After_Instruction\"]=self.refine_data[api_model]['Example']['new_prompt'][idx]['Instruction']\n",
    "                self.result[\"Ground_truth\"]=self.refine_data[api_model]['Example']['Ground_truth'][idx]\n",
    "                self.result['origin_answer']=self.refine_data[api_model]['Example']['old_Result'][idx]['Answer']\n",
    "                self.result['origin_Verbal_Confidence']=self.refine_data[api_model]['Example']['old_Result'][idx]['Confidence']\n",
    "                self.result['CAPR_answer']=self.refine_data[api_model]['Example']['Result'][idx]['Answer']\n",
    "                self.result['CAPR_Verbal_Confidence']=self.refine_data[api_model]['Example']['Result'][idx]['Confidence']\n",
    "                self.result['Accuracy']=self.refine_data[api_model]['Evaluate_result']['Accuracy'][idx]\n",
    "                self.result['old_Accuracy']=self.refine_data[api_model]['Evaluate_result']['old_Accuracy'][idx]\n",
    "                self.result['Pace_Conf']=self.refine_data[api_model]['Evaluate_result']['Pace_Conf'][idx]\n",
    "                self.result['old_Pace_Conf']=self.refine_data[api_model]['Evaluate_result']['old_Pace_Conf'][idx]\n",
    "\n",
    "    def retrieve_example(self,api_model):\n",
    "        self.Refine_exmple(api_model)\n",
    "        with open(f\"{api_model}_Example.json\",'w+') as f:\n",
    "            json.dump(self.result,f,indent=4)\n",
    "\n",
    "        # for k,v in self.result.items():\n",
    "            # print(k)\n",
    "            # print(f\"\\t{v}\")\n",
    "\n",
    "dd=demo('CAPR/Inf_din0s_asqa_r12_withPACE.json',)\n",
    "dd.retrieve_example(\"gpt-4-turbo\")\n",
    "dd.retrieve_example(\"gpt-3.5-turbo-0125\")\n",
    "dd.retrieve_example(\"claude-3-5-sonnet-20240620\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moveing Average of RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def Moving_average(data_folder):\n",
    "    data_size=20\n",
    "    ratio=0.99\n",
    "    for k in ['reward','Accuracy','ECE']:\n",
    "        data_path=f'{data_folder}/{k}.json'\n",
    "        if os.path.isfile(data_path):\n",
    "            with open(data_path,'r') as f:\n",
    "                data=json.load(f)\n",
    "            movin_avg=sum(data[:data_size])/data_size\n",
    "            k1=[]\n",
    "            for i in data:\n",
    "                movin_avg=ratio*movin_avg+(1-ratio)*i\n",
    "                k1.append(movin_avg)\n",
    "\n",
    "            plt.plot(range(len(k1)),k1,label=k,marker='')\n",
    "            plt.legend()\n",
    "            plt.savefig(f\"{data_path.replace(\".json\",\"mvavg.png\")}\")\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "\n",
    "Moving_average(\"CAPR/PPO_State_06122032_vanilla_f1_r12_withPACE\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from netcal.metrics import ECE\n",
    "from torch import rand\n",
    "import numpy as np\n",
    "def get_ece(y_confs,y_true):\n",
    "    y_confs=np.array([i.item() for i in y_confs])\n",
    "    y_true=np.array([i.item() for i in y_true])\n",
    "    accuracy = np.mean(y_true)\n",
    "    # y_true=np.where(y_true < accuracy,0,1) ## change to binary ## init 0.59\n",
    "    # ECE\n",
    "    n_bins = 10\n",
    "    # diagram = ReliabilityDiagram(n_bins)\n",
    "    ece = ECE(n_bins)\n",
    "    ece_score = ece.measure(y_confs, y_true)\n",
    "    # print(\"ECE:\", ece_score)\n",
    "    return torch.tensor(ece_score)\n",
    "\n",
    "y_true=rand(size=(128,))/10+0.5\n",
    "y_conf=rand(size=(128,))/10+0.8\n",
    "\n",
    "ece=torch.abs(y_true-y_conf)\n",
    "print(-torch.mean(ece))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import evaluate\n",
    "def calculate_rouge(reference, hypothesis):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, hypothesis)\n",
    "    print(scores)\n",
    "    return scores['rougeL'].fmeasure\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "reference = [\"The cat sat on the mat.\"]\n",
    "hypothesis = [\"The cat sat in the rain\"]\n",
    "scores = [calculate_rouge(i,j) for i,j in zip(reference, hypothesis)]\n",
    "print(scores)\n",
    "\n",
    "\n",
    "# rouge=evaluate.load('rouge')\n",
    "\n",
    "# rouge_results=[rouge.compute(predictiopni,j) for i,j in zip(hypothesis,reference)]\n",
    "# print(rouge_results)\n",
    "# print(rouge_results['rougeL'])\n",
    "# print((rouge_results['rougeL'] > 0.3).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json,random,os\n",
    "\n",
    "# datapath=\"response_result/20240517/din0s_asqa_gpt-3.5-turbo-0125_cot_Long_QA_gpt-3.5-turbo-0125_bertscore.json\"\n",
    "\n",
    "def get_datapath(dataset,api_model,activate_time,acc_model,sim_model,prompt_strategy,isshuffle_str)-> list:\n",
    "\n",
    "\n",
    "    task=[\"QA\",\"Long_QA\"]\n",
    "    datapath=[]\n",
    "    for i in prompt_strategy:\n",
    "        for t in task:\n",
    "            path=f\"response_result/{activate_time}/{dataset}_{api_model}_{i}_{t}_{sim_model}_{acc_model}_{isshuffle_str}.json\"\n",
    "            print(path)\n",
    "            if os.path.isfile(path):\n",
    "                datapath.append(path)\n",
    "\n",
    "    return datapath\n",
    "\n",
    "def mean(data):\n",
    "    return sum(data)/len(data)\n",
    "\n",
    "def Load_data(datapath):\n",
    "    with open(datapath,'r') as f:\n",
    "        data=json.load(f)\n",
    "\n",
    "    conf=[i['Confidence'] for i in data]\n",
    "    ## MAX\n",
    "    simi=[max(map(float,i['Doc_Ans_simi'])) for i in data]\n",
    "    ## Mean\n",
    "    mean_simi=[sum(map(float,i['Doc_Ans_simi']))/len(i['Doc_Ans_simi']) for i in data]\n",
    "    acc=[i['Accuracy'] for i in data]\n",
    "    assert len(simi)==len(conf)\n",
    "    return [conf,mean_simi,acc]\n",
    "\n",
    "# color=['lightblue','lightred',\"lightgreen\",\"yellow\",'pink','lightbrown']\n",
    "def Get_histogram(datalist,dataset,title,stretagy):\n",
    "# Generate sample data\n",
    "    # data1 = np.random.normal(0, 1, 1000)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    colors=plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    # assert len(lable)==len(datalist)\n",
    "    # Plot histograms\n",
    "    for idx,i in enumerate(datalist):\n",
    "        plt.hist(i, bins=100, alpha=0.7, label=stretagy[idx],color=colors[idx% len(colors)])\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f\"{dataset}_{title}\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlim([0, 1])\n",
    "    # plt.ylim([0, 600])\n",
    "    # Show the plot\n",
    "    plt.savefig(f\"picture/histogram/{dataset}_{title}.png\")\n",
    "    # plt.show()\n",
    "    plt.clf\n",
    "\n",
    "def convert_binary_acc(y_true):\n",
    "    mean_acc=[np.mean(np.array(i)) for i in y_true] ## change to binary\n",
    "\n",
    "    y_true=[np.where(np.array(i) < mean_acc[idx],0,1) for idx,i in enumerate(y_true)] ## change to binary\n",
    "    return y_true\n",
    "\n",
    "\n",
    "def show_histogram_graph(vector,title,File_name,stretagy=\"\",sim=\"\",datafile_name=\"\",label=[]):\n",
    "    os.makedirs(f\"PACE/picture/histogram/{File_name}\",exist_ok=True)\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    colors=plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    random.shuffle(colors)\n",
    "    for i,j in zip(vector,label):\n",
    "        plt.hist(i, bins=100, density=True, alpha=0.7, color=colors[random.randint(0,len(colors)-1)], edgecolor='black',label=j)\n",
    "    # Add a title and labels\n",
    "    plt.title(f'{title}')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    # plt.ylim(0,1)\n",
    "    plt.xlim(0,1)\n",
    "    # Add a grid\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='upper right')\n",
    "    # Show plot\n",
    "    plt.savefig(f\"PACE/picture/histogram/{File_name}/{datafile_name}.png\")\n",
    "    # plt.show()\n",
    "    plt.clf\n",
    "\n",
    "def show_plot_graph(vector,title,File_name,stretagy=\"\",sim=\"\",datafile_name=\"\",label=\"\"):\n",
    "    os.makedirs(f\"PACE/picture/histogram/{File_name}\",exist_ok=True)\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    colors=plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    random.shuffle(colors)\n",
    "    # vector=sorted(vector)\n",
    "    counts, bin_edges = np.histogram(vector, bins=len(vector)//10)\n",
    "\n",
    "    # 計算每個柱的中心點\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    # plt.hist(vector, bins=30, alpha=0.3, color='gray', edgecolor='black')\n",
    "    plt.plot(bin_centers,counts, marker='.',linestyle='-', color=colors[random.randint(0,len(colors)-1)],label=label)\n",
    "    # Add a title and labels\n",
    "    plt.title(f'{title}')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    # plt.ylim(0,1)\n",
    "    # Add a grid\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='upper right')\n",
    "    # Show plot\n",
    "    plt.savefig(f\"PACE/picture/histogram/{File_name}/{datafile_name}.png\")\n",
    "    # plt.show()\n",
    "    plt.clf\n",
    "\n",
    "\n",
    "def Update_Fig(activate_time,shuffle):\n",
    "    isshuffle_str=\"shuffle\" if shuffle else \"No_shuffle\"\n",
    "    datapaht=f\"./PACE/response_result/Evaluate_Result_{activate_time}_{isshuffle_str}.json\"\n",
    "    with open(datapaht,'r') as f:\n",
    "        data=json.load(f)\n",
    "\n",
    "    os.makedirs(\"PACE/picture/histogram\",exist_ok=True)\n",
    "    simi_models=[\"Cos_sim\"]\n",
    "    datasets=[\"din0s/asqa\"]\n",
    "    api_model='gpt-3.5-turbo-0125'\n",
    "    acc_model='rougeL'\n",
    "\n",
    "    stretagy=[\"vanilla\",'cot',\"multi_step\"]\n",
    "\n",
    "    for sim in simi_models:\n",
    "        for dataset in datasets:\n",
    "            for dd in data:\n",
    "                for k in stretagy:\n",
    "                    # conf_list,Final_conf_list,simi_list,acc_list=[],[],[],[]\n",
    "                    if dd['dataset']==dataset and dd['sim_model']==sim and dd['Stratagy']==k and dd['acc_model']==acc_model:\n",
    "                        print(f\"Load Sucess {dataset} {k} {sim}\")\n",
    "                        print(f\"Accuracy {np.mean(np.array(dd['Accuracy']))}\")\n",
    "                        print(f\"ECE {np.mean(np.array(dd['ece']))}\")\n",
    "                        print(f\"AUROC {np.mean(np.array(dd['auroc']))}\")\n",
    "                        dataset_path=dataset.replace(\"/\",\"_\")\n",
    "                        print(dd['Simi'])\n",
    "                        show_plot_graph(dd['Conf'],File_name=f\"{activate_time}_{isshuffle_str}\",title=f\"{k} {sim} Confidence\",stretagy=k,sim=f\"{sim}\",datafile_name=f\"{dataset_path}_{isshuffle_str}_{sim}_{k}_Confidence\",label=f\"{isshuffle_str}\")\n",
    "\n",
    "                        show_plot_graph(dd['Simi'],File_name=f\"{activate_time}_{isshuffle_str}\",title=f\"{k} {sim} Similarity\",stretagy=k,sim=f\"{sim}\",datafile_name=f\"{dataset_path}_{isshuffle_str}_{sim}_{k}_Similarity\",label=f\"{isshuffle_str}\")\n",
    "\n",
    "                        show_plot_graph(dd['Pace_Conf'],File_name=f\"{activate_time}_{isshuffle_str}\",title=f\"{k} {sim} Final Confidence\",stretagy=k,sim=f\"{sim}\",datafile_name=f\"{dataset_path}_{isshuffle_str}_{sim}_{k}_PACE_Confidence\",label=f\"{isshuffle_str}\")\n",
    "\n",
    "                        show_plot_graph(dd['Accuracy'],File_name=f\"{activate_time}_{isshuffle_str}\",title=f\"{k} {sim} Accuracy\",stretagy=k,sim=f\"{sim}\",datafile_name=f\"{dataset_path}_{isshuffle_str}_{sim}_{k}_Accuracy\",label=f\"{isshuffle_str}\")\n",
    "\n",
    "Update_Fig(\"20240601\",False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json,random,os\n",
    "\n",
    "def show_four_plot(vectorlist,title):\n",
    "    colors=plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    random.shuffle(colors)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    fig.patch.set_facecolor('lightgrey')\n",
    "    # Plot each vector list in its respective subplot\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        counts, bin_edges = np.histogram(vectorlist[i], bins=len(vectorlist[i])//10)\n",
    "        # 計算每個柱的中心點\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        # plt.hist(vector, bins=30, alpha=0.3, color='gray', edgecolor='black')\n",
    "\n",
    "        ax.plot(bin_centers,counts, marker='.',linestyle='-', color=colors[random.randint(0,len(colors)-1)],label='No Shuffle')\n",
    "        # ax.set_xlabel('Index')\n",
    "        if i%2:\n",
    "            ax.set_ylabel('Accuracy',fontsize=12)\n",
    "        else:\n",
    "            ax.set_ylabel('Confidence',fontsize=12)\n",
    "        ax.set_xlim((0,1))\n",
    "        ax.set_facecolor('whitesmoke')\n",
    "    # fig.suptitle(title, fontsize=16)\n",
    "    # Adjust layout to prevent overlapping\n",
    "    plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def Update_Fig(activate_time,shuffle):\n",
    "    isshuffle_str=\"shuffle\" if shuffle else \"No_shuffle\"\n",
    "    datapaht=f\"./PACE/response_result/Evaluate_Result_{activate_time}_{isshuffle_str}.json\"\n",
    "    with open(datapaht,'r') as f:\n",
    "        data=json.load(f)\n",
    "\n",
    "    os.makedirs(\"PACE/picture/histogram\",exist_ok=True)\n",
    "    simi_models=[\"Cos_sim\"]\n",
    "    datasets=[\"din0s/asqa\"]\n",
    "    api_model='gpt-3.5-turbo-0125'\n",
    "    acc_model='rougeL'\n",
    "\n",
    "    stretagy=[\"vanilla\",'cot',\"multi_step\"]\n",
    "    vector=[]\n",
    "    for sim in simi_models:\n",
    "        for dataset in datasets:\n",
    "            for dd in data:\n",
    "                for k in stretagy:\n",
    "                    # conf_list,Final_conf_list,simi_list,acc_list=[],[],[],[]\n",
    "                    if dd['dataset']==dataset and dd['sim_model']==sim and dd['Stratagy']==k and dd['acc_model']==acc_model:\n",
    "                        vector+=[dd['Conf'],dd['Accuracy']]\n",
    "                        show_four_plot([dd['Conf'],dd['Accuracy']],k)\n",
    "\n",
    "Update_Fig(20240601,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_histogram_graph(vector,dim_x_y,stretagy=\"\"):\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.hist(vector, bins=30, density=True, alpha=0.7, color='blue', edgecolor='black')\n",
    "    # Add a title and labels\n",
    "    plt.title(f'{stretagy} Score')\n",
    "    plt.xlabel('Confidence Value')\n",
    "    plt.ylabel('Density')\n",
    "    # plt.ylim(0,1)\n",
    "    plt.xlim(0,1)\n",
    "    # Add a grid\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show plot\n",
    "    # plt.savefig(f\"picture/histogram/{dataset}_{title}.png\")\n",
    "    plt.show()\n",
    "    plt.clf\n",
    "\n",
    "from netcal.metrics import ECE\n",
    "\n",
    "activate_time=\"20240529\"\n",
    "\n",
    "os.makedirs(\"picture/histogram\",exist_ok=True)\n",
    "simi_models=[\"Cos_sim\",\"snli\"]\n",
    "datasets=[\"natural_questions\",'din0s_asqa']\n",
    "api_model='gpt-3.5-turbo-0125'\n",
    "acc_model='bertscore'\n",
    "\n",
    "stretagy=[\"vanilla\",'cot',\"multi_step\"]\n",
    "\n",
    "simi_model=\"snli\"\n",
    "dataset=\"din0s_asqa\"\n",
    "\n",
    "path_list=get_datapath(dataset=dataset,api_model=api_model,activate_time=activate_time,acc_model=acc_model,sim_model=simi_model,prompt_strategy=stretagy)\n",
    "\n",
    "evaldata=list(map(Load_data,path_list))\n",
    "\n",
    "for k in range(3):\n",
    "    accuracy=np.mean(np.array(evaldata[k][2]))\n",
    "    y_confs=evaldata[k][0]\n",
    "    y_true=list(map(float,evaldata[k][2]))\n",
    "    y_true=np.where(np.array(y_true) < 0.9,0,1)\n",
    "\n",
    "    ll=0.5\n",
    "    pace_conf_array = np.add(ll*np.array(evaldata[k][0]),(1-ll)*np.array(evaldata[k][1]))\n",
    "\n",
    "    # ECE\n",
    "    n_bins = 10\n",
    "    # diagram = ReliabilityDiagram(n_bins)\n",
    "    ece = ECE(n_bins)\n",
    "    assert len(y_confs)==len(y_true)\n",
    "    ece_score = ece.measure(np.array(y_confs), np.array(y_true),uncertainty='mean')\n",
    "    print(\"ECE:\", ece_score)\n",
    "\n",
    "    n_bins = 10\n",
    "    # diagram = ReliabilityDiagram(n_bins)\n",
    "    ece = ECE(n_bins)\n",
    "    ece_pace_score = ece.measure(pace_conf_array, np.array(y_true))\n",
    "    print(\"ECE_PACE:\", ece_pace_score)\n",
    "\n",
    "    print(f\"conf origin {np.mean(y_confs)}, PACE {np.mean(pace_conf_array)}\")\n",
    "\n",
    "    show_histogram_graph(y_confs,dim_x_y=[0,1],stretagy=f\"{simi_model}\")\n",
    "\n",
    "    show_histogram_graph(evaldata[k][1],dim_x_y=[0,1],stretagy=f\"{simi_model}\")\n",
    "\n",
    "    show_histogram_graph(pace_conf_array,dim_x_y=[0,1],stretagy=f\"{simi_model}\")\n",
    "\n",
    "    # show_histogram_graph(y_true,dim_x_y=[0,1],stretagy=f\"{simi_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "def split_text_into_fixed_length_parts(text, tokens_per_part, model_name='bert'):\n",
    "\n",
    "    model_huggingface={\n",
    "        'bert':'bert-base-uncased',\n",
    "        'xbert':'efederici/sentence-bert-base',\n",
    "    }\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_huggingface[model_name])\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Initialize the list to hold each part\n",
    "    parts = []\n",
    "\n",
    "    # Calculate the number of full parts\n",
    "    full_parts = len(tokens) // tokens_per_part\n",
    "\n",
    "    # Create parts with exactly tokens_per_part tokens\n",
    "    for i in range(full_parts):\n",
    "        start_index = i * tokens_per_part\n",
    "        end_index = start_index + tokens_per_part\n",
    "        part_tokens = tokens[start_index:end_index]\n",
    "        # Convert token list to string and add to the parts list\n",
    "        parts.append(tokenizer.convert_tokens_to_string(part_tokens))\n",
    "\n",
    "    # Handle the remaining tokens, if any\n",
    "    if len(tokens) % tokens_per_part:\n",
    "        remaining_tokens = tokens[full_parts * tokens_per_part:]\n",
    "        parts.append(tokenizer.convert_tokens_to_string(remaining_tokens))\n",
    "\n",
    "    return parts\n",
    "\n",
    "# Example usage\n",
    "text = \"Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.\"\n",
    "tokens_per_part = 96\n",
    "result = split_text_into_fixed_length_parts(text, tokens_per_part,'bert')\n",
    "result = split_text_into_fixed_length_parts(text, tokens_per_part,'xbert')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "from evaluate import load\n",
    "def load_data():\n",
    "    with open('/home/thesis/confidence_Score/response_result/gpt-3.5-turbo-0125_vanilla_simi_2024_05_08.json','r') as f:\n",
    "        data=json.load(f)\n",
    "    # print(data.keys())\n",
    "    simi_res=data['When did the kokoda war start and end?']['similarity_res']\n",
    "    olddat=simi_res[0]\n",
    "    for i in simi_res:\n",
    "        if i[1] > olddat[1]:\n",
    "            olddat=i\n",
    "    ans,long_ans=olddat[2],olddat[3]\n",
    "    return ans,long_ans\n",
    "def Bernormalize(ans,long_ans):\n",
    "    nomalizer=BertNormalizer(clean_text=True,lowercase=True,handle_chinese_chars=True)\n",
    "    return nomalizer.normalize_str(ans),nomalizer.normalize_str(long_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WER metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans,long_ans=load_data()\n",
    "print(ans)\n",
    "print(long_ans)\n",
    "wer_metric = load(\"wer\")\n",
    "acc_wer = wer_metric.compute(references=[ans], predictions=[long_ans])\n",
    "print(f\"WER acc : {acc_wer}\")\n",
    "print(f\"1- WER acc : {1-acc_wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans,long_ans=load_data()\n",
    "print(ans)\n",
    "print(long_ans)\n",
    "exact_match_metric = load(\"exact_match\")\n",
    "results = exact_match_metric.compute(predictions=[ans], references=[long_ans])\n",
    "print(results['exact_match'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "ans,long_ans=load_data()\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embedding_1= model.encode(ans, convert_to_tensor=True)\n",
    "embedding_2 = model.encode(long_ans, convert_to_tensor=True)\n",
    "\n",
    "result=util.pytorch_cos_sim(embedding_1, embedding_2)\n",
    "\n",
    "print(result.item())\n",
    "## tensor([[0.6003]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "from evaluate import load\n",
    "ans,long_ans=load_data()\n",
    "print(ans)\n",
    "print(long_ans)\n",
    "\n",
    "\n",
    "P, R, F1 = score([ans], [long_ans],lang=\"en\",verbose=True)\n",
    "# Print scores\n",
    "print(\"Precision: \", P.item())\n",
    "print(\"Recall: \", R.item())\n",
    "print(\"F1 Score: \", F1.item())\n",
    "\n",
    "\n",
    "bertscore = load(\"bertscore\")\n",
    "result=bertscore.compute(predictions=[ans],references=[long_ans],lang='en',verbose=True)\n",
    "print(\"Precision: \", result['precision'].pop())\n",
    "print(\"Recall: \", result['recall'].pop())\n",
    "print(\"F1 Score: \", result['f1'].pop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import load_checkpoint\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "datapath=f'response_result/gpt-3.5-turbo-0125_vanilla_2024_05_09.json'\n",
    "datadict=load_checkpoint(datapath)\n",
    "\n",
    "class eval_dataloader:\n",
    "    def __init__(self,dataset_path,batch_size=1,purpose='eval') -> None:\n",
    "        self.dataset = load_checkpoint(dataset_path)\n",
    "        if purpose==\"eval\":\n",
    "            self.loader=DataLoader(list(self.dataset.values()),batch_size=batch_size,collate_fn=self.simi_acc_collate_fn,shuffle=True)\n",
    "        elif purpose==\"ece\":\n",
    "            self.loader=DataLoader(list(self.dataset.values()),batch_size=batch_size,collate_fn=self.ece_collate_fn,shuffle=False)\n",
    "\n",
    "    def simi_acc_collate_fn(self,batch):\n",
    "        res=[]\n",
    "        for i in batch:\n",
    "            res.append([i['Question'],i['Document'],i['Answer'],i['Long Answer'],i['Confidence']])\n",
    "        return res\n",
    "\n",
    "    def ece_collate_fn(self,batch):\n",
    "        simi_res=[]\n",
    "        conf_res=[]\n",
    "        accres=[]\n",
    "        for i in batch:\n",
    "            simi_res.append(i['similarity_res'])\n",
    "            conf_res.append(i['confidence'])\n",
    "            accres.append(i['acc'])\n",
    "        return simi_res,conf_res,accres\n",
    "\n",
    "\n",
    "def conf_calibration(simi,conf):\n",
    "    x_lambda=0.5\n",
    "    return x_lambda*simi+(1-x_lambda)*conf\n",
    "\n",
    "def ece_calibration(simi:list,acc:list,conf:list): # batch b_m\n",
    "    conf=list(map(conf_calibration,simi,conf))\n",
    "    assert len(acc)==len(simi)\n",
    "    b_m=len(acc)\n",
    "    ece=np.mean(np.array(acc)-np.array(conf))/b_m\n",
    "    return ece\n",
    "\n",
    "def get_most_high_simi(simi_res:list):\n",
    "    return sorted(simi_res,key=lambda x:x[1],reverse=True)[0][1]\n",
    "\n",
    "eve_l=eval_dataloader(datapath,2,'eval').loader\n",
    "print(len(eve_l))\n",
    "# for simi_res,conf_res,accres in eve_l:\n",
    "    # most_simi=list(map(get_most_high_simi,simi_res))\n",
    "    # print(ece_calibration(most_simi,accres,conf_res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "\n",
    "def compute_macro_f1(sentence1, sentence2):\n",
    "    # Tokenize the sentences\n",
    "    tokens1 = sentence1.split()\n",
    "    tokens2 = sentence2.split()\n",
    "\n",
    "    # Count the frequency of each token in both sentences\n",
    "    counter1 = Counter(tokens1)\n",
    "    counter2 = Counter(tokens2)\n",
    "\n",
    "    # Create a union of all unique tokens\n",
    "    all_tokens = list(set(tokens1) | set(tokens2))\n",
    "\n",
    "    # Create binary vectors for comparison\n",
    "    vector1 = torch.tensor([counter1[token] for token in all_tokens])\n",
    "    vector2 = torch.tensor([counter2[token] for token in all_tokens])\n",
    "\n",
    "    # Compute the macro-F1 score\n",
    "    f1 = torch.tensor(f1_score(vector1.numpy(), vector2.numpy(), average='macro'),dtype=torch.float16)\n",
    "\n",
    "    return f1\n",
    "\n",
    "# Example sentences\n",
    "sentence1 = \"The quick brown fox jumps over the lazy dog\"\n",
    "sentence2 = \"A quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "macro_f1 = compute_macro_f1(sentence1, sentence2)\n",
    "print(\"Macro F1 Score:\", macro_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Money Spent on API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob\n",
    "from util import load_checkpoint\n",
    "def Calulate_spent(file_path='response_result/gpt-3.5-turbo-0125',api_model=\"gpt-3.5-turbo-0125\"):\n",
    "    compete_toekn=0\n",
    "    prompt_token=0\n",
    "\n",
    "    for datapath in glob.glob(f'{file_path}*.json'):\n",
    "        if os.path.isfile(datapath):\n",
    "            datares=load_checkpoint(datapath)\n",
    "            print(f\"{datapath} {len(datares)}\")\n",
    "            for i in datares.values():\n",
    "                if \"Complete_tokens\" in i:\n",
    "                    compete_toekn+=i['Complete_tokens']\n",
    "                if \"Prompt_tokens\" in i:\n",
    "                    prompt_token+=i['Prompt_tokens']\n",
    "\n",
    "    Total_Spent =compete_toekn*8.00/1000000+prompt_token*6.00/1000000\n",
    "    print(f\"Complete tokens :{compete_toekn},prompt_tokens :{prompt_token} \")\n",
    "    print(f\"Total Spent {Total_Spent} USD ; {Total_Spent*30} TWD\")\n",
    "\n",
    "\n",
    "Calulate_spent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "def is_answer_correct(question, true_answer, predicted_answer):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Get embeddings\n",
    "    true_embedding = get_embedding(true_answer, model, tokenizer)\n",
    "    predicted_embedding = get_embedding(predicted_answer, model, tokenizer)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(true_embedding.numpy(), predicted_embedding.numpy())[0][0]\n",
    "\n",
    "    # Threshold for correctness\n",
    "    threshold = 0.8\n",
    "    print(similarity)\n",
    "    return similarity >= threshold\n",
    "\n",
    "# Example question and answers\n",
    "question = \"What is the capital of France?\"\n",
    "true_answer = \"The capital of France is Paris.\"\n",
    "predicted_answer = \"Paris is the capital of France.\"\n",
    "\n",
    "is_correct = is_answer_correct(question, true_answer, predicted_answer)\n",
    "print(\"Is the answer correct?\", is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    # Simple sentence tokenizer (could be improved with more sophisticated methods)\n",
    "    return text.split('. ')\n",
    "\n",
    "def evaluate_long_form_qa(true_answer, predicted_answer):\n",
    "    # Tokenize the answers into sentences\n",
    "    true_sentences = tokenize_sentences(true_answer)\n",
    "    predicted_sentences = tokenize_sentences(predicted_answer)\n",
    "\n",
    "    # Generate binary labels\n",
    "    true_labels = [1] * len(true_sentences)  # Assume all sentences in the ground truth are correct\n",
    "    predicted_labels = [1 if sentence in true_sentences else 0 for sentence in predicted_sentences]\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Example long-form answers\n",
    "true_answer = \"The quick brown fox jumps over the lazy dog. Foxes are known for their agility. Dogs can be quite lazy at times.\"\n",
    "predicted_answer = \"The quick brown fox jumps over the lazy dog. Foxes are very agile animals. Sometimes dogs are lazy.\"\n",
    "\n",
    "precision, recall, f1 = evaluate_long_form_qa(true_answer, predicted_answer)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "eval_model = BERTScorer(\n",
    "                        model_type=\"bert-base-uncased\",  # Model type\n",
    "                        num_layers=9,  # Number of layers to use\n",
    "                        all_layers=False,  # Whether to use all layers\n",
    "                        idf=False,  # Whether to use IDF scaling\n",
    "                        batch_size=64,  # Batch size\n",
    "                        lang=None,  # Language of the texts, auto-detect based on model if None\n",
    "                        rescale_with_baseline=False,  # Whether to rescale\n",
    "                        device='cuda:1'\n",
    "                    )\n",
    "def tokenize_sentences(text):\n",
    "    # Simple sentence tokenizer (could be improved with more sophisticated methods)\n",
    "    return text.split('. ')\n",
    "true_answer = \"The quick brown fox jumps over the lazy dog. Foxes are known for their agility. Dogs can be quite lazy at times.\"\n",
    "predicted_answer = \"The quick brown fox. The quick brown fox. The quick brown fox.\"\n",
    "\n",
    "true_sentences=tokenize_sentences(true_answer)\n",
    "predicted_sentences=tokenize_sentences(predicted_answer)\n",
    "true_labels = [1] * len(true_sentences)  # Assume all sentences in the ground truth are correct\n",
    "predicted_labels = [1 if sentence in true_sentences else 0 for sentence in predicted_sentences]\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "conf_path='response_result/gpt-3.5-turbo-0125_vanilla_2024_05_09.json'\n",
    "simi_acc_path='response_result/gpt-3.5-turbo-0125_vanilla_simi_acc_2024_05_09.json'\n",
    "with open(simi_acc_path,'r') as f:\n",
    "    data1=json.load(f)\n",
    "    print(len(data1))\n",
    "    for i in data1.keys():\n",
    "        print(i)\n",
    "\n",
    "with open(conf_path,'r') as f:\n",
    "    data2=json.load(f)\n",
    "    print(len(data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def show(conf_list):\n",
    "    if isinstance(conf_list,dict):\n",
    "        print(f\"Evaluate Result:\")\n",
    "        for v in conf_list.values():\n",
    "            print(f\"      {v['Stratagy']}:\")\n",
    "            for k1,v1 in v.items():\n",
    "                print(f\"        {k1} : {v1}\")\n",
    "    elif isinstance(conf_list,list):\n",
    "        print(f\"Evaluate Result:\")\n",
    "        for v in conf_list:\n",
    "            print(f\"      {v['Stratagy']}:\")\n",
    "            for k1,v1 in v.items():\n",
    "                print(f\"        {k1} : {v1}\")\n",
    "\n",
    "for i in glob.glob(\"response_result/*_eval_*.json\"):\n",
    "    with open(i,'r') as f:\n",
    "        show(json.load(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a =[torch.tensor(i/100) for i in range(0,100)]\n",
    "print(a)\n",
    "print(torch.stack(a,dim=0))\n",
    "# print(torch.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "confidence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
