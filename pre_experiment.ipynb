{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trivia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"CAPR/Inf_trivia_r13_withPACE.json\",'r') as f:\n",
    "    data=json.load(f)\n",
    "\n",
    "api_model='gpt-3.5-turbo-0125'\n",
    "jj=[]\n",
    "# print(data[api_model]['Evaluate_result'].keys())\n",
    "# print(data[api_model]['Example']['new_prompt'])\n",
    "for idx,i in enumerate(data['gpt-3.5-turbo-0125']['Example']['new_prompt']):\n",
    "    if data[api_model]['Evaluate_result']['Accuracy'][idx] ==1:\n",
    "        # if data[api_model]['Example']['old_prompt'][idx]['Question']==\"Question : In which country is the Sky Train Rail bridge?\":\n",
    "        if data[api_model]['Example']['Ground_truth'][idx]!=data[api_model]['Example']['Result'][idx]:\n",
    "            jj.append(\n",
    "                {\n",
    "                    \"Q\":i['Question'],\n",
    "                    \"GT\":data[api_model]['Example']['Ground_truth'][idx],\n",
    "                    \"Old_instruc\":data[api_model]['Example']['old_prompt'][idx]['Instruction'],\n",
    "                    \"New_instruc\":data[api_model]['Example']['new_prompt'][idx]['Instruction'],\n",
    "                    \"Old_result\":data[api_model]['Example']['Result'][idx],\n",
    "                    \"Result\":data[api_model]['Example']['old_Result'][idx],\n",
    "                    \"Accuracy\":data[api_model]['Evaluate_result']['Accuracy'][idx]\n",
    "                }\n",
    "            )\n",
    "\n",
    "with open(\"bad_exmaple.json\",\"w\") as f:\n",
    "    json.dump(jj,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"CAPR/Inf_din0s_asqa_r12_withPACE.json\",'r') as f:\n",
    "    data=json.load(f)\n",
    "print(data['gpt-3.5-turbo-0125']['Evaluate_result'].keys())\n",
    "print(data['gpt-3.5-turbo-0125']['Example']['new_prompt'])\n",
    "for idx,i in enumerate(data['gpt-3.5-turbo-0125']['Example']['new_prompt']):\n",
    "\n",
    "    if i['Question']==\"Question : Who has the highest goals in world football?\":\n",
    "        print(i['Question'])\n",
    "        print(f\"gt: {data['gpt-3.5-turbo-0125']['Example']['Ground_truth'][idx]}\")\n",
    "        print(f\"{data['gpt-3.5-turbo-0125']['Example']['Result'][idx]}\")\n",
    "        print(data['gpt-3.5-turbo-0125']['Evaluate_result']['Accuracy'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Id', 'Original_question', 'Documnet', 'Ground_truth', 'Answer', 'Confidence', 'rouge_score'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"base_work/baseline_result/asqa_gpt-3.5-turbo-0125_vanilla.json\",'r') as f:\n",
    "    data=json.load(f)\n",
    "\n",
    "print(data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import concurrent\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import os,json,re,yaml\n",
    "import textgrad as tg\n",
    "from textgrad.tasks import load_task\n",
    "import numpy as np\n",
    "import random,json\n",
    "import qadataset_dataloader as qadataset_dataloader\n",
    "import torch\n",
    "\n",
    "\n",
    "if os.path.isfile(\"api_key.yml\"):\n",
    "    with open(\"api_key.yml\",\"r\") as f:\n",
    "        key=yaml.safe_load(f)\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = key['openai']['api_key']\n",
    "\n",
    "\n",
    "llm_api_eval = tg.get_engine(engine_name=\"gpt-4o\")\n",
    "llm_api_test = tg.get_engine(engine_name=\"gpt-3.5-turbo-0125\")\n",
    "tg.set_backward_engine(llm_api_eval, override=True)\n",
    "\n",
    "# Initialize the system prompt\n",
    "system_prompt = tg. Variable (\"You are a helpful language model,Think step by step .\",requires_grad = True ,role_description =\" system prompt to the language model \")\n",
    "\n",
    "# Set up the model object ’ parameterized by ’ the prompt .\n",
    "model = tg. BlackboxLLM ( system_prompt = system_prompt )\n",
    "\n",
    "# Optimize the system prompt\n",
    "optimizer = tg. TextualGradientDescent ( parameters =[ system_prompt ])\n",
    "for iteration in range (5):\n",
    "    question_str=\"Which poisonous compound was used by a religous cult in deadly attacks on the Tokyo subway in 1995?\"\n",
    "    answer_str=\"sarin gas\"\n",
    "\n",
    "    batch_x = tg.Variable(question_str, role_description=\"question to the LLM\", requires_grad=False)\n",
    "\n",
    "    batch_y = tg.Variable(answer_str, role_description=\"answer to the question\", requires_grad=False)\n",
    "\n",
    "    optimizer . zero_grad ()\n",
    "    # Do the forward pass\n",
    "    responses = model (batch_x)\n",
    "    losses = [ loss_fn ( response , y ) for ( response , y) in zip ( responses , batch_y ) ]\n",
    "    total_loss = tg. sum( losses )\n",
    "    # Perform the backward pass and compute gradients\n",
    "    total_loss . backward ()\n",
    "    # Update the system prompt\n",
    "    optimizer . step ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import concurrent\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import os,json,re,yaml\n",
    "import textgrad as tg\n",
    "from textgrad.tasks import load_task\n",
    "import numpy as np\n",
    "import random,json\n",
    "\n",
    "\n",
    "question_str=\"Which poisonous compound was used by a religous cult in deadly attacks on the Tokyo subway in 1995?\"\n",
    "answer_str=\"sarin gas\"\n",
    "\n",
    "class text_grad:\n",
    "    def __init__(self) -> None:\n",
    "        load_dotenv(override=True)\n",
    "        llm_api_eval = tg.get_engine(engine_name=\"gpt-4o\")\n",
    "        llm_api_test = tg.get_engine(engine_name=\"gpt-3.5-turbo-0125\")\n",
    "        tg.set_backward_engine(llm_api_eval, override=True)\n",
    "        train_set, val_set, test_set, self.eval_fn = load_task(\"BBH_word_sorting\", evaluation_api=llm_api_eval)\n",
    "\n",
    "        self.STARTING_SYSTEM_PROMPT=\"You will answer a reasoning question with confidence score. Think step by step. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is the answer to the question\"\n",
    "        self.set_up_keys()\n",
    "\n",
    "        self.system_prompt_eval = tg.Variable(self.STARTING_SYSTEM_PROMPT,\n",
    "                            requires_grad=True,\n",
    "                            role_description=\"system prompt to the language model\")\n",
    "        self.model_evaluation = tg.BlackboxLLM(llm_api_eval, self.system_prompt_eval)\n",
    "\n",
    "        self.system_prompt = tg.Variable(self.STARTING_SYSTEM_PROMPT,\n",
    "                                    requires_grad=True,\n",
    "                                    role_description=\"structured system prompt to a somewhat capable language model that specifies the behavior and strategies for the QA task\")\n",
    "        self.model = tg.BlackboxLLM(llm_api_test, self.system_prompt)\n",
    "\n",
    "        self.optimizer = tg.TextualGradientDescent(engine=llm_api_eval, parameters=[self.system_prompt])\n",
    "\n",
    "    def set_up_keys(self):\n",
    "        if os.path.isfile(\"api_key.yml\"):\n",
    "            with open(\"api_key.yml\",\"r\") as f:\n",
    "                key=yaml.safe_load(f)\n",
    "\n",
    "        os.environ['OPENAI_API_KEY'] = key['openai']['api_key']\n",
    "\n",
    "    def generate(self,question_str,answer_str):\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        question = tg.Variable(question_str, role_description=\"question to the LLM\", requires_grad=False)\n",
    "\n",
    "        answer = tg.Variable(answer_str, role_description=\"answer to the question\", requires_grad=False)\n",
    "        prediction_origin = self.model(question)\n",
    "        print(prediction_origin.__repr__())\n",
    "        loss = self.eval_fn(inputs=dict(prediction=prediction_origin, ground_truth_answer=answer))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        prediction_after = self.model(question)\n",
    "        print(prediction_after.__repr__())\n",
    "        result={\n",
    "            \"prompt_before\":self.STARTING_SYSTEM_PROMPT,\n",
    "            'prompt_after':self.system_prompt.value,\n",
    "            'answer_before':prediction_origin.value,\n",
    "            # 'grad_fn_before':prediction_origin.get_grad_fn(),\n",
    "            'answer_after':prediction_after.value,\n",
    "            \"ground_truth\":answer.value,\n",
    "            # 'grad_fn_after':prediction_origin.get_grad_fn(),\n",
    "        }\n",
    "        return result\n",
    "\n",
    "tt=text_grad()\n",
    "result=tt.generate(question_str,answer_str)\n",
    "with open('text.json','w') as f:\n",
    "    json.dump(result,f,indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import concurrent\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import os,json,re,yaml\n",
    "import textgrad as tg\n",
    "from textgrad.tasks import load_task\n",
    "import numpy as np\n",
    "import random,json\n",
    "\n",
    "class text_grad:\n",
    "    def __init__(self) -> None:\n",
    "        self.set_up_keys()\n",
    "        tg.set_backward_engine(\"gpt-3.5-turbo-0125\", override=True)\n",
    "\n",
    "        # Step 1: Get an initial response from an LLM.\n",
    "        self.model = tg.BlackboxLLM(\"gpt-4o\")\n",
    "    def set_up_keys(self):\n",
    "        if os.path.isfile(\"api_key.yml\"):\n",
    "            with open(\"api_key.yml\",\"r\") as f:\n",
    "                key=yaml.safe_load(f)\n",
    "\n",
    "        os.environ['OPENAI_API_KEY'] = key['openai']['api_key']\n",
    "\n",
    "    def parser(self,text):\n",
    "        answer_match = re.search(r'\"answer\": \"(.*?)\"', text)\n",
    "        if answer_match:\n",
    "            answer = answer_match.group(1)\n",
    "        # Regex to capture the confidence score\n",
    "        confidence_match = re.search(r'\"confidence\": (\\d+\\.\\d+)', text)\n",
    "        if confidence_match:\n",
    "            confidence_score = float(confidence_match.group(1))\n",
    "\n",
    "        return {\"Answer\":answer,\"Confidence\":confidence_score}\n",
    "\n",
    "    def text_grad_get_response(self,question_str,answer_str):\n",
    "\n",
    "        question_string = (f\"{question_str}\"\n",
    "                        \"provide 'answer' to the question and 'confidence' to the answer in json,confidence is a float value between 0 and 1\")\n",
    "\n",
    "        question = tg.Variable(question_string,\n",
    "                            role_description=\"question to the LLM\",\n",
    "                            requires_grad=False)\n",
    "\n",
    "        answer = self.model(question)\n",
    "\n",
    "        print(answer)\n",
    "        result=self.parser(str(answer))\n",
    "\n",
    "        answer.set_role_description(\"concise and accurate answer to the question\")\n",
    "\n",
    "        # Step 2: Define the loss function and the optimizer, just like in PyTorch!\n",
    "        # Here, we don't have SGD, but we have TGD (Textual Gradient Descent)\n",
    "        # that works with \"textual gradients\".\n",
    "        optimizer = tg.TGD(parameters=[answer])\n",
    "        evaluation_instruction = (f\"Here's a question: {question_string}. \"\n",
    "                                \"Evaluate any given answer to this question, \"\n",
    "                                \"be smart, logical, and very critical. \"\n",
    "                                \"Just provide concise feedback.\"\n",
    "                                )\n",
    "        # TextLoss is a natural-language specified loss function that describes\n",
    "        # how we want to evaluate the reasoning.\n",
    "        loss_fn = tg.TextLoss(evaluation_instruction)\n",
    "\n",
    "        # Step 3: Do the loss computation, backward pass, and update the punchline.\n",
    "        # Exact same syntax as PyTorch!\n",
    "        loss = loss_fn(answer)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(answer)\n",
    "        result=self.parser(answer.value)\n",
    "\n",
    "        return result\n",
    "\n",
    "question_str=\"Who is the best golfer in the world?\"\n",
    "answer_str=\"\"\n",
    "result=text_grad().text_grad_get_response(question_str,answer_str)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import randint\n",
    "class demo:\n",
    "    def __init__(self,path) -> None:\n",
    "        with open(path,\"r\") as f:\n",
    "            self.refine_data=json.load(f)\n",
    "\n",
    "    def Get_original_result(self,Question):\n",
    "        with open('CAPR/response_result/20240601/din0s_asqa_gpt-3.5-turbo-0125_vanilla_Long_QA.json','r') as af:\n",
    "            self.origin_data=json.load(af)\n",
    "\n",
    "        print(self.origin_data[0].keys())\n",
    "\n",
    "        for i in self.origin_data:\n",
    "            if i[\"Question\"]==Question:\n",
    "                self.result['origin_answer']=i[\"Answer\"]\n",
    "                self.result['origin_confidence']=i[\"Confidence\"]\n",
    "                self.result['origin_Instruction']=i['Prompt'][\"Instruction\"]\n",
    "\n",
    "    def Refine_exmple(self,api_model):\n",
    "        temp_acc=0.1\n",
    "        self.result={}\n",
    "        data_size=len(self.refine_data[api_model]['Evaluate_result']['Accuracy'])\n",
    "        print(f\"{api_model} Data size {data_size}\")\n",
    "        idx=randint(0,data_size-1)\n",
    "        for idx in range(data_size):\n",
    "            if self.refine_data[api_model]['Example']['old_prompt'][idx]['Question']==\"Question : When was the first apple i phone made?\":\n",
    "                # print(self.refine_data[api_model]['Example'].keys())\n",
    "                self.result[\"Question\"]=self.refine_data[api_model]['Example']['old_prompt'][idx]['Question']\n",
    "                self.result[\"Origin_Instruction\"]=self.refine_data[api_model]['Example']['old_prompt'][idx]['Instruction']\n",
    "                self.result[\"After_Instruction\"]=self.refine_data[api_model]['Example']['new_prompt'][idx]['Instruction']\n",
    "                self.result[\"Ground_truth\"]=self.refine_data[api_model]['Example']['Ground_truth'][idx]\n",
    "                self.result['origin_answer']=self.refine_data[api_model]['Example']['old_Result'][idx]['Answer']\n",
    "                self.result['origin_Verbal_Confidence']=self.refine_data[api_model]['Example']['old_Result'][idx]['Confidence']\n",
    "                self.result['CAPR_answer']=self.refine_data[api_model]['Example']['Result'][idx]['Answer']\n",
    "                self.result['CAPR_Verbal_Confidence']=self.refine_data[api_model]['Example']['Result'][idx]['Confidence']\n",
    "                self.result['Accuracy']=self.refine_data[api_model]['Evaluate_result']['Accuracy'][idx]\n",
    "                self.result['old_Accuracy']=self.refine_data[api_model]['Evaluate_result']['old_Accuracy'][idx]\n",
    "                self.result['Pace_Conf']=self.refine_data[api_model]['Evaluate_result']['Pace_Conf'][idx]\n",
    "                self.result['old_Pace_Conf']=self.refine_data[api_model]['Evaluate_result']['old_Pace_Conf'][idx]\n",
    "\n",
    "    def retrieve_example(self,api_model):\n",
    "        self.Refine_exmple(api_model)\n",
    "        with open(f\"{api_model}_Example.json\",'w+') as f:\n",
    "            json.dump(self.result,f,indent=4)\n",
    "\n",
    "        # for k,v in self.result.items():\n",
    "            # print(k)\n",
    "            # print(f\"\\t{v}\")\n",
    "\n",
    "dd=demo('CAPR/Inf_din0s_asqa_r12_withPACE.json',)\n",
    "dd.retrieve_example(\"gpt-4-turbo\")\n",
    "dd.retrieve_example(\"gpt-3.5-turbo-0125\")\n",
    "dd.retrieve_example(\"claude-3-5-sonnet-20240620\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moveing Average of RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def Moving_average(data_folder):\n",
    "    data_size=20\n",
    "    ratio=0.99\n",
    "    for k in ['reward','Accuracy','ECE']:\n",
    "        data_path=f'{data_folder}/{k}.json'\n",
    "        if os.path.isfile(data_path):\n",
    "            with open(data_path,'r') as f:\n",
    "                data=json.load(f)\n",
    "            movin_avg=sum(data[:data_size])/data_size\n",
    "            k1=[]\n",
    "            for i in data:\n",
    "                movin_avg=ratio*movin_avg+(1-ratio)*i\n",
    "                k1.append(movin_avg)\n",
    "\n",
    "            plt.plot(range(len(k1)),k1,label=k,marker='')\n",
    "            plt.legend()\n",
    "            plt.savefig(f\"{data_path.replace(\".json\",\"mvavg.png\")}\")\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "\n",
    "Moving_average(\"CAPR/PPO_State_06122032_vanilla_f1_r1_trivia_withPACE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from netcal.metrics import ECE\n",
    "from torch import rand\n",
    "import numpy as np\n",
    "def get_ece(y_confs,y_true):\n",
    "    y_confs=np.array([i.item() for i in y_confs])\n",
    "    y_true=np.array([i.item() for i in y_true])\n",
    "    accuracy = np.mean(y_true)\n",
    "    # y_true=np.where(y_true < accuracy,0,1) ## change to binary ## init 0.59\n",
    "    # ECE\n",
    "    n_bins = 10\n",
    "    # diagram = ReliabilityDiagram(n_bins)\n",
    "    ece = ECE(n_bins)\n",
    "    ece_score = ece.measure(y_confs, y_true)\n",
    "    # print(\"ECE:\", ece_score)\n",
    "    return torch.tensor(ece_score)\n",
    "\n",
    "y_true=rand(size=(128,))/10+0.5\n",
    "y_conf=rand(size=(128,))/10+0.8\n",
    "\n",
    "ece=torch.abs(y_true-y_conf)\n",
    "print(-torch.mean(ece))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['gpt-3.5-turbo-0125', 'gpt-4-turbo', 'claude-3-5-sonnet-20240620'])\n",
      "gpt-3.5-turbo-0125\n",
      "dict_keys(['pace_ece', 'Verbalized_ece', 'Accuracy', 'Pace_Conf', 'Verbalized_conf', 'auroc', 'capr_auroc', 'old_pace_ece', 'old_Verbalized_ece', 'old_Accuracy', 'old_Pace_Conf', 'old_Verbalized_conf', 'old_auroc', 'old_capr_auroc'])\n",
      "0.031\n",
      "0.033\n",
      "gpt-4-turbo\n",
      "dict_keys(['pace_ece', 'Verbalized_ece', 'Accuracy', 'Pace_Conf', 'Verbalized_conf', 'auroc', 'capr_auroc', 'old_pace_ece', 'old_Verbalized_ece', 'old_Accuracy', 'old_Pace_Conf', 'old_Verbalized_conf', 'old_auroc', 'old_capr_auroc'])\n",
      "0.027\n",
      "0.029\n",
      "claude-3-5-sonnet-20240620\n",
      "dict_keys(['pace_ece', 'Verbalized_ece', 'Accuracy', 'Pace_Conf', 'Verbalized_conf', 'auroc', 'capr_auroc', 'old_pace_ece', 'old_Verbalized_ece', 'old_Accuracy', 'old_Pace_Conf', 'old_Verbalized_conf', 'old_auroc', 'old_capr_auroc'])\n",
      "0.027\n",
      "0.031\n",
      "dict_keys(['gpt-3.5-turbo-0125', 'gpt-4-turbo', 'claude-3-5-sonnet-20240620'])\n",
      "gpt-3.5-turbo-0125\n",
      "dict_keys(['pace_ece', 'Verbalized_ece', 'Accuracy', 'Pace_Conf', 'Verbalized_conf', 'auroc', 'capr_auroc', 'old_pace_ece', 'old_Verbalized_ece', 'old_Accuracy', 'old_Pace_Conf', 'old_Verbalized_conf', 'old_auroc', 'old_capr_auroc'])\n",
      "0.184\n",
      "0.173\n",
      "gpt-4-turbo\n",
      "dict_keys(['pace_ece', 'Verbalized_ece', 'Accuracy', 'Pace_Conf', 'Verbalized_conf', 'auroc', 'capr_auroc', 'old_pace_ece', 'old_Verbalized_ece', 'old_Accuracy', 'old_Pace_Conf', 'old_Verbalized_conf', 'old_auroc', 'old_capr_auroc'])\n",
      "0.166\n",
      "0.146\n",
      "claude-3-5-sonnet-20240620\n",
      "dict_keys(['pace_ece', 'Verbalized_ece', 'Accuracy', 'Pace_Conf', 'Verbalized_conf', 'auroc', 'capr_auroc', 'old_pace_ece', 'old_Verbalized_ece', 'old_Accuracy', 'old_Pace_Conf', 'old_Verbalized_conf', 'old_auroc', 'old_capr_auroc'])\n",
      "0.216\n",
      "0.190\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import scipy.stats as st\n",
    "\n",
    "def TrustRegion(gfg_data):\n",
    "    trust_region=st.t.interval(confidence=0.90,\n",
    "                    df=len(gfg_data)-1,\n",
    "                    loc=np.mean(gfg_data),\n",
    "                    scale=st.sem(gfg_data))\n",
    "    # print(f\"{trust_region}\")\n",
    "    trust_value=trust_region[1]-trust_region[0]\n",
    "    print(f\"{trust_value:.3f}\")\n",
    "    return trust_region\n",
    "with open('CAPR/Inf_din0s_asqa_r12_withPACE.json','r') as f:\n",
    "    data=json.load(f)\n",
    "print(data.keys())\n",
    "for k,v in data.items():\n",
    "    print(f\"{k}\")\n",
    "    print(v['Evaluate_result'].keys())\n",
    "    TrustRegion(v['Evaluate_result']['Accuracy'])\n",
    "    TrustRegion(v['Evaluate_result']['Verbalized_ece'])\n",
    "\n",
    "\n",
    "with open('CAPR/Inf_trivia_r13_withPACE.json','r') as f:\n",
    "    data=json.load(f)\n",
    "print(data.keys())\n",
    "for k,v in data.items():\n",
    "    print(f\"{k}\")\n",
    "    print(v['Evaluate_result'].keys())\n",
    "    TrustRegion(v['Evaluate_result']['Accuracy'])\n",
    "    TrustRegion(v['Evaluate_result']['Verbalized_ece'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import evaluate\n",
    "def calculate_rouge(reference, hypothesis):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, hypothesis)\n",
    "    print(scores)\n",
    "    return scores['rougeL'].fmeasure\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "reference = [\"The cat sat on the mat.\"]\n",
    "hypothesis = [\"The cat sat in the rain\"]\n",
    "scores = [calculate_rouge(i,j) for i,j in zip(reference, hypothesis)]\n",
    "print(scores)\n",
    "\n",
    "\n",
    "# rouge=evaluate.load('rouge')\n",
    "\n",
    "# rouge_results=[rouge.compute(predictiopni,j) for i,j in zip(hypothesis,reference)]\n",
    "# print(rouge_results)\n",
    "# print(rouge_results['rougeL'])\n",
    "# print((rouge_results['rougeL'] > 0.3).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc,roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "def Getfig(fpr,tpr,auc,data_name):\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    # plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(f\"picture/{data_name}_roc_curve.png\")\n",
    "    # plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def aur_trail(activate_time,shuffle):\n",
    "    isshuffle_str=\"shuffle\" if shuffle else \"No_shuffle\"\n",
    "    datapaht=f\"./PACE/response_result/Evaluate_Result_{activate_time}_{isshuffle_str}.json\"\n",
    "    with open(datapaht,'r') as f:\n",
    "        data=json.load(f)\n",
    "    simi_models=[\"Cos_sim\"]\n",
    "    datasets=[\"triviaQA\"]\n",
    "    api_model='gpt-3.5-turbo-0125'\n",
    "    acc_model='bool_acc'\n",
    "    stretagy=[\"vanilla\",'cot',\"multi_step\"]\n",
    "\n",
    "    for sim in simi_models:\n",
    "        for dataset in datasets:\n",
    "            for dd in data:\n",
    "                for k in stretagy:\n",
    "                    # conf_list,Final_conf_list,simi_list,acc_list=[],[],[],[]\n",
    "                    if dd['dataset']==dataset and dd['sim_model']==sim and dd['Stratagy']==k and dd['acc_model']==acc_model and dd['api_model']==api_model:\n",
    "                        print(\"*\"*100)\n",
    "                        print(f\"Load Sucess {dataset} {k} {sim} {acc_model}\")\n",
    "                        acc=np.array(dd['Accuracy'])\n",
    "                        accurate=np.where(acc < 0.55,0,1)\n",
    "                        score1=np.array(dd['Conf'])\n",
    "                        score2=np.array(dd['Pace_Conf'])\n",
    "                        fpr1, tpr1, thresholds1 = roc_curve(accurate, score1)\n",
    "                        fpr2, tpr2, thresholds2 = roc_curve(accurate, score2)\n",
    "                        auc1=auc(fpr1, tpr1)\n",
    "                        auc2=auc(fpr2, tpr2)\n",
    "                        print(np.mean(accurate))\n",
    "                        print(np.mean(score2))\n",
    "                        # print(score2)\n",
    "                        print(auc1,auc2)\n",
    "                        Getfig(fpr1,tpr1,auc1,f\"{dataset}\")\n",
    "                        Getfig(fpr2,tpr2,auc2,dataset)\n",
    "\n",
    "aur_trail(activate_time=\"20240601\",shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET PACE Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import json\n",
    "import numpy as np\n",
    "def Print_data(activate_time,shuffle):\n",
    "    isshuffle_str=\"shuffle\" if shuffle else \"No_shuffle\"\n",
    "    datapaht=f\"./PACE/response_result/Evaluate_Result_{activate_time}_{isshuffle_str}.json\"\n",
    "    with open(datapaht,'r') as f:\n",
    "        data=json.load(f)\n",
    "    simi_models=[\"Cos_sim\"]\n",
    "    datasets=[\"triviaQA\"]\n",
    "    api_model='claude-3-5-sonnet-20240620'\n",
    "    acc_model='bool_acc'\n",
    "    stretagy=[\"vanilla\",'cot',\"multi_step\"]\n",
    "\n",
    "    for sim in simi_models:\n",
    "        for dataset in datasets:\n",
    "            for dd in data:\n",
    "                for k in stretagy:\n",
    "                    # conf_list,Final_conf_list,simi_list,acc_list=[],[],[],[]\n",
    "                    if dd['dataset']==dataset and dd['sim_model']==sim and dd['Stratagy']==k and dd['acc_model']==acc_model and dd['api_model']==api_model:\n",
    "                        print(\"*\"*100)\n",
    "                        print(f\"Load Sucess {dataset} {k} {sim} {acc_model}\")\n",
    "                        print(f\"Accuracy {np.mean(np.array(dd['Accuracy']))}\")\n",
    "                        print(f\"ECE {np.mean(np.array(dd['ece']))}\")\n",
    "                        print(f\"ECE after {np.mean(np.array(dd['ece_pace']))}\")\n",
    "                        print(f\"AUROC {np.mean(np.array(dd['auroc']))}\")\n",
    "                        print(f\"AUROC after {np.mean(np.array(dd['auroc_pace']))}\")\n",
    "                        # print(\"*\"*100)\n",
    "Print_data(activate_time=\"20240601\",shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Objective histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Sucess din0s/asqa vanilla Cos_sim\n",
      "Load Sucess din0s/asqa cot Cos_sim\n",
      "Load Sucess din0s/asqa multi_step Cos_sim\n",
      "Load Sucess triviaQA vanilla Cos_sim\n",
      "Load Sucess triviaQA cot Cos_sim\n",
      "Load Sucess triviaQA multi_step Cos_sim\n",
      "['din0s/asqa', 'triviaQA']\n",
      "['vanilla', 'cot', 'multi_step']\n",
      "{'vanilla': {'ece': [0.7285499999999436, 0.3221059516023719], 'ece_pace': [0.657930950125859, 0.22344758071984785], 'auroc': [0.5255151865433674, 0.7012522361359572], 'auroc_pace': [0.6120642375873961, 0.6904837430610626]}, 'cot': {'ece': [0.8119499999999518, 0.29556338028170304], 'ece_pace': [0.7091389551553134, 0.20305949403009704], 'auroc': [0.5260424132740825, 0.721549711263259], 'auroc_pace': [0.7246281335113246, 0.712530669355246]}, 'multi_step': {'ece': [0.8006324999999874, 0.3051712779973785], 'ece_pace': [0.7058935563853604, 0.2119777643483756], 'auroc': [0.5038863556481309, 0.71541101527366], 'auroc_pace': [0.6760309752570657, 0.6862145426131903]}}\n"
     ]
    }
   ],
   "source": [
    "import json,os\n",
    "def clean_data(accuracy,conf):\n",
    "    clean_acc,clean_conf=[],[]\n",
    "    for ac,cof in zip(accuracy,conf):\n",
    "        if cof < 0.5:\n",
    "            print(ac,cof)\n",
    "        else:\n",
    "            clean_acc.append(ac)\n",
    "            clean_conf.append(cof)\n",
    "    return np.array(clean_acc),np.array(clean_conf)\n",
    "\n",
    "def get_value(datadd,datadict,stretagy):\n",
    "\n",
    "    for k,v in datadict[stretagy].items():\n",
    "        if k in datadd:\n",
    "            datadict[stretagy][k].append(datadd[k])\n",
    "    return datadict\n",
    "\n",
    "def init(stretagy):\n",
    "    pace_data={}\n",
    "    for i in stretagy:\n",
    "        pace_data[i]={\n",
    "            \"ece\":[],\n",
    "            \"ece_pace\":[],\n",
    "            'auroc':[],\n",
    "            'auroc_pace':[]\n",
    "        }\n",
    "    return pace_data\n",
    "\n",
    "def Update_histogram(activate_time,shuffle):\n",
    "    isshuffle_str=\"shuffle\" if shuffle else \"No_shuffle\"\n",
    "    datapaht=f\"./PACE/response_result/Evaluate_Result_{activate_time}_{isshuffle_str}.json\"\n",
    "    with open(datapaht,'r') as f:\n",
    "        data=json.load(f)\n",
    "\n",
    "    os.makedirs(\"PACE/picture/histogram\",exist_ok=True)\n",
    "    simi_models=[\"Cos_sim\"]\n",
    "    datasets=[\"din0s/asqa\",\"triviaQA\"] ## Label\n",
    "    api_model='gpt-3.5-turbo-0125'\n",
    "\n",
    "    acc_mapping={\"din0s/asqa\":\"rougeL\",\"triviaQA\":\"f1\"}\n",
    "    label_mapping={\"din0s/asqa\":\"ASQA\",\"triviaQA\":\"TriviaQA\"}\n",
    "\n",
    "    stretagy=[\"vanilla\",'cot',\"multi_step\"] ## category_type\n",
    "    pace_data=init(stretagy)\n",
    "    for sim in simi_models:\n",
    "        for idx,dataset in enumerate(datasets):\n",
    "            for dd in data:\n",
    "                for k in stretagy:\n",
    "                    # conf_list,Final_conf_list,simi_list,acc_list=[],[],[],[]\n",
    "                    if dd['dataset']==dataset and dd['sim_model']==sim and dd['Stratagy']==k and dd['acc_model']==acc_mapping[dataset] and api_model==dd['api_model']:\n",
    "                        dataset_path=dataset.replace(\"/\",\"_\")\n",
    "                        print(f\"Load Sucess {dataset} {k} {sim}\")\n",
    "                        # acc,pace_conf=clean_data(dd['Accuracy'],dd['Pace_Conf'])\n",
    "                        # acc,conf=clean_data(dd['Accuracy'],dd['Conf'])\n",
    "                        pace_data=get_value(dd,pace_data,k)\n",
    "\n",
    "    return datasets,stretagy,pace_data\n",
    "\n",
    "\n",
    "label,stretagy,pace_data=Update_histogram(\"20240601\",False)\n",
    "\n",
    "print(label)\n",
    "print(stretagy)\n",
    "print(pace_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ece'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Show the plot\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 29\u001b[0m project_objective_histogram(label,stretagy,\u001b[43mpace_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mece\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mECE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# project_objective_histogram(label,stretagy,before_pace['auroc'],\"\",\"AUROC\")\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ece'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def project_objective_histogram(labels,strategy, category_data, x_title, y_title):\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.2  # the width of the bars\n",
    "    num_categories = len(strategy)\n",
    "    offset = (num_categories - 1) / 2  # Center the bars around the ticks\n",
    "    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    print(category_data)\n",
    "    # Plotting the bars\n",
    "    for idx, data in enumerate(category_data):\n",
    "        rects = ax.bar(x - offset * width + idx * width, data, width, label=strategy[idx%len(strategy)], color=colors[idx])\n",
    "\n",
    "    # Add some text for labels, title, and custom x-axis tick labels, etc.\n",
    "    ax.set_xlabel(f'{x_title}')\n",
    "    ax.set_ylabel(f'{y_title}')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0,1])\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "project_objective_histogram(label,stretagy,pace_data['ece'],\"\",\"ECE\")\n",
    "# project_objective_histogram(label,stretagy,before_pace['auroc'],\"\",\"AUROC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "labels = ['A', 'B', 'C', 'D']\n",
    "category1 = [3, 2, 5, 7]\n",
    "category2 = [2, 3, 2, 4]\n",
    "category3 = [1, 4, 3, 3]\n",
    "category4 = [4, 3, 2, 1]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.2  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plotting the bars\n",
    "rects1 = ax.bar(x - 1.5*width, category1, width, label='Category 1', color='orange')\n",
    "rects2 = ax.bar(x - 0.5*width, category2, width, label='Category 2', color='gold')\n",
    "rects3 = ax.bar(x + 0.5*width, category3, width, label='Category 3', color='red')\n",
    "rects4 = ax.bar(x + 1.5*width, category4, width, label='Category 4', color='gray')\n",
    "\n",
    "# Add some text for labels, title, and custom x-axis tick labels, etc.\n",
    "ax.set_xlabel('Labels')\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_title('Grouped bar chart')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json,random,os\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "# datapath=\"response_result/20240517/din0s_asqa_gpt-3.5-turbo-0125_cot_Long_QA_gpt-3.5-turbo-0125_bertscore.json\"\n",
    "\n",
    "def get_datapath(dataset,api_model,activate_time,acc_model,sim_model,prompt_strategy,isshuffle_str)-> list:\n",
    "    task=[\"QA\",\"Long_QA\"]\n",
    "    datapath=[]\n",
    "    for i in prompt_strategy:\n",
    "        for t in task:\n",
    "            path=f\"response_result/{activate_time}/{dataset}_{api_model}_{i}_{t}_{sim_model}_{acc_model}_{isshuffle_str}.json\"\n",
    "            print(path)\n",
    "            if os.path.isfile(path):\n",
    "                datapath.append(path)\n",
    "\n",
    "    return datapath\n",
    "\n",
    "def mean(data):\n",
    "    return sum(data)/len(data)\n",
    "\n",
    "def Load_data(datapath):\n",
    "    with open(datapath,'r') as f:\n",
    "        data=json.load(f)\n",
    "\n",
    "    conf=[i['Confidence'] for i in data]\n",
    "    ## MAX\n",
    "    simi=[max(map(float,i['Doc_Ans_simi'])) for i in data]\n",
    "    ## Mean\n",
    "    mean_simi=[sum(map(float,i['Doc_Ans_simi']))/len(i['Doc_Ans_simi']) for i in data]\n",
    "    acc=[i['Accuracy'] for i in data]\n",
    "    assert len(simi)==len(conf)\n",
    "    return [conf,mean_simi,acc]\n",
    "\n",
    "\n",
    "# color=['lightblue','lightred',\"lightgreen\",\"yellow\",'pink','lightbrown']\n",
    "def Get_histogram(datalist,dataset,title,stretagy):\n",
    "# Generate sample data\n",
    "    # data1 = np.random.normal(0, 1, 1000)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    colors=plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    # assert len(lable)==len(datalist)\n",
    "    # Plot histograms\n",
    "    for idx,i in enumerate(datalist):\n",
    "        plt.hist(i, bins=100, alpha=0.7, label=stretagy[idx],color=colors[idx% len(colors)])\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f\"{dataset}_{title}\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlim([0, 1])\n",
    "    # plt.ylim([0, 600])\n",
    "    # Show the plot\n",
    "    plt.savefig(f\"picture/histogram/{dataset}_{title}.png\")\n",
    "    # plt.show()\n",
    "    plt.clf\n",
    "\n",
    "def show_histogram_graph(vector,title,File_name,stretagy=\"\",sim=\"\",datafile_name=\"\",label=[]):\n",
    "    os.makedirs(f\"PACE/picture/histogram/{File_name}\",exist_ok=True)\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    colors=plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    random.shuffle(colors)\n",
    "    for i,j in zip(vector,label):\n",
    "        plt.hist(i, bins=100, density=True, alpha=0.7, color=colors[random.randint(0,len(colors)-1)], edgecolor='black',label=j)\n",
    "    # Add a title and labels\n",
    "    plt.title(f'{title}')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    # plt.ylim(0,1)\n",
    "    plt.xlim(0,1)\n",
    "    # Add a grid\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='upper right')\n",
    "    # Show plot\n",
    "    plt.savefig(f\"PACE/picture/histogram/{File_name}/{datafile_name}.png\")\n",
    "    # plt.show()\n",
    "    plt.clf\n",
    "\n",
    "def show_plot_graph(vector,File_name,datafile_name=\"\",label=\"\"):\n",
    "    os.makedirs(f\"PACE/picture/histogram/{File_name}\",exist_ok=True)\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(6, 3),facecolor='none')\n",
    "    colors=plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    random.shuffle(colors)\n",
    "    ## len(vector)//5\n",
    "    counts, bin_edges = np.histogram(vector, bins=10)\n",
    "    plt.gca().set_facecolor(None)\n",
    "    # 計算每個柱的中心點\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    # plt.hist(vector, bins=30, alpha=0.3, color='gray', edgecolor='black')\n",
    "    plt.plot(bin_centers,counts, marker='',linestyle='-', color=colors[random.randint(0,len(colors)-1)],label=label,linewidth=3)\n",
    "    # Add a grid\n",
    "    # plt.grid(True)\n",
    "    plt.xlim(0,1)\n",
    "    plt.legend(loc='upper left', fontsize=10)\n",
    "    # Show plot\n",
    "    plt.savefig(f\"PACE/picture/histogram/{File_name}/{datafile_name}.png\",transparent=True)\n",
    "    # plt.show()\n",
    "    plt.clf\n",
    "\n",
    "\n",
    "def calibration_curve_fucntion_single(accuracy1,Confidence1,accuracy2,Confidence2,n_bins=10,File_name=\"\",datafile_name=\"\",labely=\"\",labelx=\"\",label_legend=\"\",color_index=0):\n",
    "    os.makedirs(f\"PACE/picture/histogram/{File_name}\",exist_ok=True)\n",
    "    # Calculate the calibration curve\n",
    "    accuracy1=np.where(np.array(accuracy1)<0.3,0,1)\n",
    "    accuracy2=np.where(np.array(accuracy2)<0.3,0,1)\n",
    "    prob_true1, prob_pred1 = calibration_curve(accuracy1,Confidence1, n_bins=n_bins)\n",
    "    prob_true2, prob_pred2 = calibration_curve(accuracy2,Confidence2, n_bins=n_bins)\n",
    "\n",
    "    # Plot the reliability diagram\n",
    "    plt.figure(figsize=(4, 4),facecolor='none',dpi=300)\n",
    "    colors=plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    print(colors)\n",
    "    # random.shuffle(colors)\n",
    "\n",
    "    random_color=random.randint(0,len(colors)-1)\n",
    "\n",
    "    plt.plot(prob_pred1,prob_true1 , marker='^', label=f'{label_legend} calibration curve',color=colors[color_index%len(colors)],linewidth=2,markersize=7)\n",
    "    # plt.plot(prob_pred2,prob_true2 , marker='o', label=f'{label_legend} w/ PACE',color=colors[(color_index+1)%len(colors)],linewidth=2,markersize=8)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated',color='black',linewidth=2)\n",
    "    plt.gca().set_facecolor(None)\n",
    "    plt.xlabel(f'{labelx}',fontsize=7)\n",
    "    plt.ylabel(f'Accuracy',fontsize=7)\n",
    "    plt.xlim(-0.05,1.05)\n",
    "    plt.ylim(-0.05,1.05)\n",
    "\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid()\n",
    "    # plt.show()\n",
    "    plt.savefig(f\"PACE/picture/histogram/{File_name}/{datafile_name}_single.png\",transparent=True)\n",
    "    plt.clf\n",
    "\n",
    "\n",
    "def calibration_curve_fucntion(accuracy1,Confidence1,accuracy2,Confidence2,n_bins=10,File_name=\"\",datafile_name=\"\",labely=\"\",labelx=\"\",label_legend=\"\",color_index=0):\n",
    "    os.makedirs(f\"PACE/picture/histogram/{File_name}\",exist_ok=True)\n",
    "    # Calculate the calibration curve\n",
    "    accuracy1=np.where(np.array(accuracy1)<0.3,0,1)\n",
    "    accuracy2=np.where(np.array(accuracy2)<0.3,0,1)\n",
    "    prob_true1, prob_pred1 = calibration_curve(accuracy1,Confidence1, n_bins=n_bins)\n",
    "    prob_true2, prob_pred2 = calibration_curve(accuracy2,Confidence2, n_bins=n_bins)\n",
    "\n",
    "    # Plot the reliability diagram\n",
    "    plt.figure(figsize=(4, 4),facecolor='none',dpi=500)\n",
    "    colors=plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    print(colors)\n",
    "    # random.shuffle(colors)\n",
    "\n",
    "    random_color=random.randint(0,len(colors)-1)\n",
    "\n",
    "    plt.plot(prob_pred1,prob_true1 , marker='^', label=f'{label_legend} w/o PACE',color=colors[color_index%len(colors)],linewidth=2,markersize=7)\n",
    "    plt.plot(prob_pred2,prob_true2 , marker='o', label=f'{label_legend} w/ PACE',color=colors[(color_index+1)%len(colors)],linewidth=2,markersize=7)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated',color='black',linewidth=2)\n",
    "    plt.gca().set_facecolor(None)\n",
    "    plt.xlabel(f'{labelx}',fontsize=7)\n",
    "    plt.ylabel(f'Accuracy',fontsize=7)\n",
    "    plt.xlim(-0.05,1.05)\n",
    "    plt.ylim(-0.05,1.05)\n",
    "\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid()\n",
    "    # plt.show()\n",
    "    plt.savefig(f\"PACE/picture/histogram/{File_name}/{datafile_name}.png\",transparent=True)\n",
    "    plt.clf\n",
    "\n",
    "\n",
    "def clean_data(accuracy,conf):\n",
    "    clean_acc,clean_conf=[],[]\n",
    "    for ac,cof in zip(accuracy,conf):\n",
    "        if cof < 0.5:\n",
    "            print(ac,cof)\n",
    "        else:\n",
    "            clean_acc.append(ac)\n",
    "            clean_conf.append(cof)\n",
    "    return np.array(clean_acc),np.array(clean_conf)\n",
    "\n",
    "def Update_Fig(activate_time,shuffle):\n",
    "    isshuffle_str=\"shuffle\" if shuffle else \"No_shuffle\"\n",
    "    datapaht=f\"./PACE/response_result/Evaluate_Result_{activate_time}_{isshuffle_str}.json\"\n",
    "    with open(datapaht,'r') as f:\n",
    "        data=json.load(f)\n",
    "\n",
    "    os.makedirs(\"PACE/picture/histogram\",exist_ok=True)\n",
    "    simi_models=[\"Cos_sim\"]\n",
    "    datasets=[\"din0s/asqa\",\"triviaQA\"]\n",
    "    # datasets=[\"triviaQA\"]\n",
    "    api_model='gpt-3.5-turbo-0125'\n",
    "\n",
    "    acc_mapping={\"din0s/asqa\":\"rougeL\",\"triviaQA\":\"f1\"}\n",
    "    label_mapping={\"din0s/asqa\":\"ASQA\",\"triviaQA\":\"TriviaQA\"}\n",
    "\n",
    "    # stretagy=[\"vanilla\",'cot',\"multi_step\"]\n",
    "    stretagy=[\"vanilla\"]\n",
    "    color_index=2\n",
    "    for sim in simi_models:\n",
    "        for idx,dataset in enumerate(datasets):\n",
    "            color_index+=2\n",
    "            for dd in data:\n",
    "                for k in stretagy:\n",
    "                    # conf_list,Final_conf_list,simi_list,acc_list=[],[],[],[]\n",
    "                    if dd['dataset']==dataset and dd['sim_model']==sim and dd['Stratagy']==k and dd['acc_model']==acc_mapping[dataset] and api_model==dd['api_model']:\n",
    "                        dataset_path=dataset.replace(\"/\",\"_\")\n",
    "                        print(f\"Load Sucess {dataset} {k} {sim}\")\n",
    "\n",
    "                        acc,pace_conf=clean_data(dd['Accuracy'],dd['Pace_Conf'])\n",
    "                        acc,conf=clean_data(dd['Accuracy'],dd['Conf'])\n",
    "\n",
    "                        ### Show plot Figure\n",
    "                        show_plot_graph(dd['Conf'],File_name=f\"{activate_time}_{isshuffle_str}\",datafile_name=f\"{dataset_path}_{isshuffle_str}_{sim}_{k}_Confidence\",label=f\"Confidence\")\n",
    "\n",
    "                        show_plot_graph(dd['Simi'],File_name=f\"{activate_time}_{isshuffle_str}\",datafile_name=f\"{dataset_path}_{isshuffle_str}_{sim}_{k}_Similarity\",label=f\"Similarity\")\n",
    "\n",
    "                        show_plot_graph(dd['Pace_Conf'],File_name=f\"{activate_time}_{isshuffle_str}\",datafile_name=f\"{dataset_path}_{isshuffle_str}_{sim}_{k}_PACE_Confidence\",label=f\"PACE Confidence\")\n",
    "\n",
    "                        show_plot_graph(dd['Accuracy'],File_name=f\"{activate_time}_{isshuffle_str}\",datafile_name=f\"{dataset_path}_{isshuffle_str}_{sim}_{k}_Accuracy\",label=f\"Accuracy\")\n",
    "\n",
    "                        ### Show Accuracy and Confidence figure\n",
    "                        calibration_curve_fucntion(acc,conf,acc,pace_conf,40,File_name=f\"{activate_time}_{isshuffle_str}\",datafile_name=f\"{dataset_path}_calibration_curve\",labely=f\"{acc_mapping[dataset]}\",labelx=\"Confidence\",label_legend=f\"{label_mapping[dataset]}\",color_index=color_index)\n",
    "                        print(\"*\"*100)\n",
    "                        calibration_curve_fucntion_single(acc,conf,acc,pace_conf,40,File_name=f\"{activate_time}_{isshuffle_str}\",datafile_name=f\"{dataset_path}_calibration_curve\",labely=f\"{acc_mapping[dataset]}\",labelx=\"Confidence\",label_legend=f\"{label_mapping[dataset]}\",color_index=color_index)\n",
    "\n",
    "Update_Fig(\"20240601\",False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Sample data: probabilities and actual outcomes\n",
    "# Example data, replace with your own model predictions and true labels\n",
    "y_prob = confidence\n",
    "y_true = np.random.randint(0, 2, 1000)\n",
    "y_true=np.where(accuracy < 0.3,0,1)\n",
    "\n",
    "\n",
    "def calibration_curve_fucntion(y_true,y_prob,n_bins=10):\n",
    "    # Calculate the calibration curve\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins)\n",
    "\n",
    "    # Plot the reliability diagram\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label='Reliability curve')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated')\n",
    "\n",
    "    plt.xlabel('Confidence',fontsize=12)\n",
    "    plt.ylabel('Accuracy',fontsize=12)\n",
    "    # plt.title('Reliability Diagram')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bins_result(accuracy,confidence,n_bins=20):\n",
    "    bins=np.linspace(0,1,n_bins+1)\n",
    "    bin_acc=np.zeros(n_bins)\n",
    "    bin_conf=np.zeros(n_bins)\n",
    "    bin_count=np.zeros(n_bins)\n",
    "    bin_acc_result,bin_conf_result=[],[]\n",
    "    for ac,cof in zip(accuracy,confidence):\n",
    "        for idx in range(n_bins):\n",
    "            if ac>=bins[idx] and ac<bins[idx+1]:\n",
    "                bin_acc[idx]+=ac\n",
    "                bin_conf[idx]+=cof\n",
    "                bin_count[idx]+=1\n",
    "\n",
    "    print(len(bin_count),len(bin_acc),len(bin_conf))\n",
    "    for bsum,bccount in zip(bin_acc,bin_count):\n",
    "        if bccount!=0:\n",
    "            bin_acc_result.append(bsum/bccount)\n",
    "\n",
    "    for bsum,bccount in zip(bin_conf,bin_count):\n",
    "        if bccount!=0:\n",
    "            bin_conf_result.append(bsum/bccount)\n",
    "\n",
    "    return bin_acc_result,bin_conf_result\n",
    "\n",
    "\n",
    "bin_acc_result,bin_conf_result=get_bins_result(accuracy,confidence)\n",
    "pace_bin_acc_result,pace_bin_conf_result=get_bins_result(accuracy,pace_confidence)\n",
    "print(len(pace_bin_conf_result))\n",
    "print(pace_bin_conf_result)\n",
    "print(pace_bin_acc_result)\n",
    "# acc_conf_plot(binacc,bin_conf,f\"text\",'asqa','vanilla')\n",
    "acc_conf_plot(bin_acc_result,bin_conf_result,f\"text\",'asqa','vanilla')\n",
    "acc_conf_plot(bin_acc_result,pace_bin_conf_result,f\"text\",'asqa','vanilla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json,random,os\n",
    "\n",
    "def show_four_plot(vectorlist,title):\n",
    "    colors=plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    random.shuffle(colors)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    fig.patch.set_facecolor('lightgrey')\n",
    "    # Plot each vector list in its respective subplot\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        counts, bin_edges = np.histogram(vectorlist[i], bins=len(vectorlist[i])//10)\n",
    "        # 計算每個柱的中心點\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        # plt.hist(vector, bins=30, alpha=0.3, color='gray', edgecolor='black')\n",
    "\n",
    "        ax.plot(bin_centers,counts, marker='.',linestyle='-', color=colors[random.randint(0,len(colors)-1)],label='No Shuffle')\n",
    "        # ax.set_xlabel('Index')\n",
    "        if i%2:\n",
    "            ax.set_ylabel('Accuracy',fontsize=12)\n",
    "        else:\n",
    "            ax.set_ylabel('Confidence',fontsize=12)\n",
    "        ax.set_xlim((0,1))\n",
    "        ax.set_facecolor('whitesmoke')\n",
    "    # fig.suptitle(title, fontsize=16)\n",
    "    # Adjust layout to prevent overlapping\n",
    "    plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def Update_Fig(activate_time,shuffle):\n",
    "    isshuffle_str=\"shuffle\" if shuffle else \"No_shuffle\"\n",
    "    datapaht=f\"./PACE/response_result/Evaluate_Result_{activate_time}_{isshuffle_str}.json\"\n",
    "    with open(datapaht,'r') as f:\n",
    "        data=json.load(f)\n",
    "\n",
    "    os.makedirs(\"PACE/picture/histogram\",exist_ok=True)\n",
    "    simi_models=[\"Cos_sim\"]\n",
    "    datasets=[\"din0s/asqa\"]\n",
    "    api_model='gpt-3.5-turbo-0125'\n",
    "    acc_model='rougeL'\n",
    "\n",
    "    stretagy=[\"vanilla\",'cot',\"multi_step\"]\n",
    "    vector=[]\n",
    "    for sim in simi_models:\n",
    "        for dataset in datasets:\n",
    "            for dd in data:\n",
    "                for k in stretagy:\n",
    "                    # conf_list,Final_conf_list,simi_list,acc_list=[],[],[],[]\n",
    "                    if dd['dataset']==dataset and dd['sim_model']==sim and dd['Stratagy']==k and dd['acc_model']==acc_model:\n",
    "                        vector+=[dd['Conf'],dd['Accuracy']]\n",
    "                        show_four_plot([dd['Conf'],dd['Accuracy']],k)\n",
    "\n",
    "Update_Fig(20240601,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_histogram_graph(vector,dim_x_y,stretagy=\"\"):\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.hist(vector, bins=30, density=True, alpha=0.7, color='blue', edgecolor='black')\n",
    "    # Add a title and labels\n",
    "    plt.title(f'{stretagy} Score')\n",
    "    plt.xlabel('Confidence Value')\n",
    "    plt.ylabel('Density')\n",
    "    # plt.ylim(0,1)\n",
    "    plt.xlim(0,1)\n",
    "    # Add a grid\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show plot\n",
    "    # plt.savefig(f\"picture/histogram/{dataset}_{title}.png\")\n",
    "    plt.show()\n",
    "    plt.clf\n",
    "\n",
    "from netcal.metrics import ECE\n",
    "\n",
    "activate_time=\"20240529\"\n",
    "\n",
    "os.makedirs(\"picture/histogram\",exist_ok=True)\n",
    "simi_models=[\"Cos_sim\",\"snli\"]\n",
    "datasets=[\"natural_questions\",'din0s_asqa']\n",
    "api_model='gpt-3.5-turbo-0125'\n",
    "acc_model='bertscore'\n",
    "\n",
    "stretagy=[\"vanilla\",'cot',\"multi_step\"]\n",
    "\n",
    "simi_model=\"snli\"\n",
    "dataset=\"din0s_asqa\"\n",
    "\n",
    "path_list=get_datapath(dataset=dataset,api_model=api_model,activate_time=activate_time,acc_model=acc_model,sim_model=simi_model,prompt_strategy=stretagy)\n",
    "\n",
    "evaldata=list(map(Load_data,path_list))\n",
    "\n",
    "for k in range(3):\n",
    "    accuracy=np.mean(np.array(evaldata[k][2]))\n",
    "    y_confs=evaldata[k][0]\n",
    "    y_true=list(map(float,evaldata[k][2]))\n",
    "    y_true=np.where(np.array(y_true) < 0.9,0,1)\n",
    "\n",
    "    ll=0.5\n",
    "    pace_conf_array = np.add(ll*np.array(evaldata[k][0]),(1-ll)*np.array(evaldata[k][1]))\n",
    "\n",
    "    # ECE\n",
    "    n_bins = 10\n",
    "    # diagram = ReliabilityDiagram(n_bins)\n",
    "    ece = ECE(n_bins)\n",
    "    assert len(y_confs)==len(y_true)\n",
    "    ece_score = ece.measure(np.array(y_confs), np.array(y_true),uncertainty='mean')\n",
    "    print(\"ECE:\", ece_score)\n",
    "\n",
    "    n_bins = 10\n",
    "    # diagram = ReliabilityDiagram(n_bins)\n",
    "    ece = ECE(n_bins)\n",
    "    ece_pace_score = ece.measure(pace_conf_array, np.array(y_true))\n",
    "    print(\"ECE_PACE:\", ece_pace_score)\n",
    "\n",
    "    print(f\"conf origin {np.mean(y_confs)}, PACE {np.mean(pace_conf_array)}\")\n",
    "\n",
    "    show_histogram_graph(y_confs,dim_x_y=[0,1],stretagy=f\"{simi_model}\")\n",
    "\n",
    "    show_histogram_graph(evaldata[k][1],dim_x_y=[0,1],stretagy=f\"{simi_model}\")\n",
    "\n",
    "    show_histogram_graph(pace_conf_array,dim_x_y=[0,1],stretagy=f\"{simi_model}\")\n",
    "\n",
    "    # show_histogram_graph(y_true,dim_x_y=[0,1],stretagy=f\"{simi_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "def split_text_into_fixed_length_parts(text, tokens_per_part, model_name='bert'):\n",
    "\n",
    "    model_huggingface={\n",
    "        'bert':'bert-base-uncased',\n",
    "        'xbert':'efederici/sentence-bert-base',\n",
    "    }\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_huggingface[model_name])\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Initialize the list to hold each part\n",
    "    parts = []\n",
    "\n",
    "    # Calculate the number of full parts\n",
    "    full_parts = len(tokens) // tokens_per_part\n",
    "\n",
    "    # Create parts with exactly tokens_per_part tokens\n",
    "    for i in range(full_parts):\n",
    "        start_index = i * tokens_per_part\n",
    "        end_index = start_index + tokens_per_part\n",
    "        part_tokens = tokens[start_index:end_index]\n",
    "        # Convert token list to string and add to the parts list\n",
    "        parts.append(tokenizer.convert_tokens_to_string(part_tokens))\n",
    "\n",
    "    # Handle the remaining tokens, if any\n",
    "    if len(tokens) % tokens_per_part:\n",
    "        remaining_tokens = tokens[full_parts * tokens_per_part:]\n",
    "        parts.append(tokenizer.convert_tokens_to_string(remaining_tokens))\n",
    "\n",
    "    return parts\n",
    "\n",
    "# Example usage\n",
    "text = \"Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.\"\n",
    "tokens_per_part = 96\n",
    "result = split_text_into_fixed_length_parts(text, tokens_per_part,'bert')\n",
    "result = split_text_into_fixed_length_parts(text, tokens_per_part,'xbert')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "from evaluate import load\n",
    "def load_data():\n",
    "    with open('/home/thesis/confidence_Score/response_result/gpt-3.5-turbo-0125_vanilla_simi_2024_05_08.json','r') as f:\n",
    "        data=json.load(f)\n",
    "    # print(data.keys())\n",
    "    simi_res=data['When did the kokoda war start and end?']['similarity_res']\n",
    "    olddat=simi_res[0]\n",
    "    for i in simi_res:\n",
    "        if i[1] > olddat[1]:\n",
    "            olddat=i\n",
    "    ans,long_ans=olddat[2],olddat[3]\n",
    "    return ans,long_ans\n",
    "def Bernormalize(ans,long_ans):\n",
    "    nomalizer=BertNormalizer(clean_text=True,lowercase=True,handle_chinese_chars=True)\n",
    "    return nomalizer.normalize_str(ans),nomalizer.normalize_str(long_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WER metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans,long_ans=load_data()\n",
    "print(ans)\n",
    "print(long_ans)\n",
    "wer_metric = load(\"wer\")\n",
    "acc_wer = wer_metric.compute(references=[ans], predictions=[long_ans])\n",
    "print(f\"WER acc : {acc_wer}\")\n",
    "print(f\"1- WER acc : {1-acc_wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans,long_ans=load_data()\n",
    "print(ans)\n",
    "print(long_ans)\n",
    "exact_match_metric = load(\"exact_match\")\n",
    "results = exact_match_metric.compute(predictions=[ans], references=[long_ans])\n",
    "print(results['exact_match'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "ans,long_ans=load_data()\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embedding_1= model.encode(ans, convert_to_tensor=True)\n",
    "embedding_2 = model.encode(long_ans, convert_to_tensor=True)\n",
    "\n",
    "result=util.pytorch_cos_sim(embedding_1, embedding_2)\n",
    "\n",
    "print(result.item())\n",
    "## tensor([[0.6003]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "from evaluate import load\n",
    "ans,long_ans=load_data()\n",
    "print(ans)\n",
    "print(long_ans)\n",
    "\n",
    "\n",
    "P, R, F1 = score([ans], [long_ans],lang=\"en\",verbose=True)\n",
    "# Print scores\n",
    "print(\"Precision: \", P.item())\n",
    "print(\"Recall: \", R.item())\n",
    "print(\"F1 Score: \", F1.item())\n",
    "\n",
    "\n",
    "bertscore = load(\"bertscore\")\n",
    "result=bertscore.compute(predictions=[ans],references=[long_ans],lang='en',verbose=True)\n",
    "print(\"Precision: \", result['precision'].pop())\n",
    "print(\"Recall: \", result['recall'].pop())\n",
    "print(\"F1 Score: \", result['f1'].pop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import load_checkpoint\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "datapath=f'response_result/gpt-3.5-turbo-0125_vanilla_2024_05_09.json'\n",
    "datadict=load_checkpoint(datapath)\n",
    "\n",
    "class eval_dataloader:\n",
    "    def __init__(self,dataset_path,batch_size=1,purpose='eval') -> None:\n",
    "        self.dataset = load_checkpoint(dataset_path)\n",
    "        if purpose==\"eval\":\n",
    "            self.loader=DataLoader(list(self.dataset.values()),batch_size=batch_size,collate_fn=self.simi_acc_collate_fn,shuffle=True)\n",
    "        elif purpose==\"ece\":\n",
    "            self.loader=DataLoader(list(self.dataset.values()),batch_size=batch_size,collate_fn=self.ece_collate_fn,shuffle=False)\n",
    "\n",
    "    def simi_acc_collate_fn(self,batch):\n",
    "        res=[]\n",
    "        for i in batch:\n",
    "            res.append([i['Question'],i['Document'],i['Answer'],i['Long Answer'],i['Confidence']])\n",
    "        return res\n",
    "\n",
    "    def ece_collate_fn(self,batch):\n",
    "        simi_res=[]\n",
    "        conf_res=[]\n",
    "        accres=[]\n",
    "        for i in batch:\n",
    "            simi_res.append(i['similarity_res'])\n",
    "            conf_res.append(i['confidence'])\n",
    "            accres.append(i['acc'])\n",
    "        return simi_res,conf_res,accres\n",
    "\n",
    "\n",
    "def conf_calibration(simi,conf):\n",
    "    x_lambda=0.5\n",
    "    return x_lambda*simi+(1-x_lambda)*conf\n",
    "\n",
    "def ece_calibration(simi:list,acc:list,conf:list): # batch b_m\n",
    "    conf=list(map(conf_calibration,simi,conf))\n",
    "    assert len(acc)==len(simi)\n",
    "    b_m=len(acc)\n",
    "    ece=np.mean(np.array(acc)-np.array(conf))/b_m\n",
    "    return ece\n",
    "\n",
    "def get_most_high_simi(simi_res:list):\n",
    "    return sorted(simi_res,key=lambda x:x[1],reverse=True)[0][1]\n",
    "\n",
    "eve_l=eval_dataloader(datapath,2,'eval').loader\n",
    "print(len(eve_l))\n",
    "# for simi_res,conf_res,accres in eve_l:\n",
    "    # most_simi=list(map(get_most_high_simi,simi_res))\n",
    "    # print(ece_calibration(most_simi,accres,conf_res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "\n",
    "def compute_macro_f1(sentence1, sentence2):\n",
    "    # Tokenize the sentences\n",
    "    tokens1 = sentence1.split()\n",
    "    tokens2 = sentence2.split()\n",
    "\n",
    "    # Count the frequency of each token in both sentences\n",
    "    counter1 = Counter(tokens1)\n",
    "    counter2 = Counter(tokens2)\n",
    "\n",
    "    # Create a union of all unique tokens\n",
    "    all_tokens = list(set(tokens1) | set(tokens2))\n",
    "\n",
    "    # Create binary vectors for comparison\n",
    "    vector1 = torch.tensor([counter1[token] for token in all_tokens])\n",
    "    vector2 = torch.tensor([counter2[token] for token in all_tokens])\n",
    "\n",
    "    # Compute the macro-F1 score\n",
    "    f1 = torch.tensor(f1_score(vector1.numpy(), vector2.numpy(), average='macro'),dtype=torch.float16)\n",
    "\n",
    "    return f1\n",
    "\n",
    "# Example sentences\n",
    "sentence1 = \"The quick brown fox jumps over the lazy dog\"\n",
    "sentence2 = \"A quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "macro_f1 = compute_macro_f1(sentence1, sentence2)\n",
    "print(\"Macro F1 Score:\", macro_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Money Spent on API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob\n",
    "from util import load_checkpoint\n",
    "def Calulate_spent(file_path='response_result/gpt-3.5-turbo-0125',api_model=\"gpt-3.5-turbo-0125\"):\n",
    "    compete_toekn=0\n",
    "    prompt_token=0\n",
    "\n",
    "    for datapath in glob.glob(f'{file_path}*.json'):\n",
    "        if os.path.isfile(datapath):\n",
    "            datares=load_checkpoint(datapath)\n",
    "            print(f\"{datapath} {len(datares)}\")\n",
    "            for i in datares.values():\n",
    "                if \"Complete_tokens\" in i:\n",
    "                    compete_toekn+=i['Complete_tokens']\n",
    "                if \"Prompt_tokens\" in i:\n",
    "                    prompt_token+=i['Prompt_tokens']\n",
    "\n",
    "    Total_Spent =compete_toekn*8.00/1000000+prompt_token*6.00/1000000\n",
    "    print(f\"Complete tokens :{compete_toekn},prompt_tokens :{prompt_token} \")\n",
    "    print(f\"Total Spent {Total_Spent} USD ; {Total_Spent*30} TWD\")\n",
    "\n",
    "\n",
    "Calulate_spent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "def is_answer_correct(question, true_answer, predicted_answer):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Get embeddings\n",
    "    true_embedding = get_embedding(true_answer, model, tokenizer)\n",
    "    predicted_embedding = get_embedding(predicted_answer, model, tokenizer)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(true_embedding.numpy(), predicted_embedding.numpy())[0][0]\n",
    "\n",
    "    # Threshold for correctness\n",
    "    threshold = 0.8\n",
    "    print(similarity)\n",
    "    return similarity >= threshold\n",
    "\n",
    "# Example question and answers\n",
    "question = \"What is the capital of France?\"\n",
    "true_answer = \"The capital of France is Paris.\"\n",
    "predicted_answer = \"Paris is the capital of France.\"\n",
    "\n",
    "is_correct = is_answer_correct(question, true_answer, predicted_answer)\n",
    "print(\"Is the answer correct?\", is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    # Simple sentence tokenizer (could be improved with more sophisticated methods)\n",
    "    return text.split('. ')\n",
    "\n",
    "def evaluate_long_form_qa(true_answer, predicted_answer):\n",
    "    # Tokenize the answers into sentences\n",
    "    true_sentences = tokenize_sentences(true_answer)\n",
    "    predicted_sentences = tokenize_sentences(predicted_answer)\n",
    "\n",
    "    # Generate binary labels\n",
    "    true_labels = [1] * len(true_sentences)  # Assume all sentences in the ground truth are correct\n",
    "    predicted_labels = [1 if sentence in true_sentences else 0 for sentence in predicted_sentences]\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Example long-form answers\n",
    "true_answer = \"The quick brown fox jumps over the lazy dog. Foxes are known for their agility. Dogs can be quite lazy at times.\"\n",
    "predicted_answer = \"The quick brown fox jumps over the lazy dog. Foxes are very agile animals. Sometimes dogs are lazy.\"\n",
    "\n",
    "precision, recall, f1 = evaluate_long_form_qa(true_answer, predicted_answer)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "eval_model = BERTScorer(\n",
    "                        model_type=\"bert-base-uncased\",  # Model type\n",
    "                        num_layers=9,  # Number of layers to use\n",
    "                        all_layers=False,  # Whether to use all layers\n",
    "                        idf=False,  # Whether to use IDF scaling\n",
    "                        batch_size=64,  # Batch size\n",
    "                        lang=None,  # Language of the texts, auto-detect based on model if None\n",
    "                        rescale_with_baseline=False,  # Whether to rescale\n",
    "                        device='cuda:1'\n",
    "                    )\n",
    "def tokenize_sentences(text):\n",
    "    # Simple sentence tokenizer (could be improved with more sophisticated methods)\n",
    "    return text.split('. ')\n",
    "true_answer = \"The quick brown fox jumps over the lazy dog. Foxes are known for their agility. Dogs can be quite lazy at times.\"\n",
    "predicted_answer = \"The quick brown fox. The quick brown fox. The quick brown fox.\"\n",
    "\n",
    "true_sentences=tokenize_sentences(true_answer)\n",
    "predicted_sentences=tokenize_sentences(predicted_answer)\n",
    "true_labels = [1] * len(true_sentences)  # Assume all sentences in the ground truth are correct\n",
    "predicted_labels = [1 if sentence in true_sentences else 0 for sentence in predicted_sentences]\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "conf_path='response_result/gpt-3.5-turbo-0125_vanilla_2024_05_09.json'\n",
    "simi_acc_path='response_result/gpt-3.5-turbo-0125_vanilla_simi_acc_2024_05_09.json'\n",
    "with open(simi_acc_path,'r') as f:\n",
    "    data1=json.load(f)\n",
    "    print(len(data1))\n",
    "    for i in data1.keys():\n",
    "        print(i)\n",
    "\n",
    "with open(conf_path,'r') as f:\n",
    "    data2=json.load(f)\n",
    "    print(len(data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def show(conf_list):\n",
    "    if isinstance(conf_list,dict):\n",
    "        print(f\"Evaluate Result:\")\n",
    "        for v in conf_list.values():\n",
    "            print(f\"      {v['Stratagy']}:\")\n",
    "            for k1,v1 in v.items():\n",
    "                print(f\"        {k1} : {v1}\")\n",
    "    elif isinstance(conf_list,list):\n",
    "        print(f\"Evaluate Result:\")\n",
    "        for v in conf_list:\n",
    "            print(f\"      {v['Stratagy']}:\")\n",
    "            for k1,v1 in v.items():\n",
    "                print(f\"        {k1} : {v1}\")\n",
    "\n",
    "for i in glob.glob(\"response_result/*_eval_*.json\"):\n",
    "    with open(i,'r') as f:\n",
    "        show(json.load(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a =[torch.tensor(i/100) for i in range(0,100)]\n",
    "print(a)\n",
    "print(torch.stack(a,dim=0))\n",
    "# print(torch.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "confidence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
