{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import json,yaml\n",
    "with open('base_work/api_key.yml','r') as f:\n",
    "    key=yaml.safe_load(f)\n",
    "print(key['claude']['api_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer':['The song \"Baby Did a Bad Bad Thing\" was sung by American singer-songwriter Chris Isaak. It was released in 1995 as part of his sixth studio album \"Forever Blue.\" The song gained significant popularity and wider recognition when it was featured in Stanley Kubrick's 1999 film \"Eyes Wide Shut,\" starring Tom Cruise and Nicole Kidman. Chris Isaak, known for his distinctive voice and retro rock and roll style, wrote and performed this sultry, blues-influenced track. The song's provocative lyrics and Isaak's deep, brooding vocals contributed to its memorable and somewhat controversial nature. It showcases Isaak's ability to blend vintage rock sounds with contemporary production, creating a timeless quality that has helped the song endure.\"Baby Did a Bad Bad Thing\" reached number 27 on the UK Singles Chart and became one of Isaak's most recognizable hits. The music video for the song, directed by Herb Ritts, featured supermodel Helena Christensen and added to the song's sensual and mysterious aura.While Chris Isaak has had several other successful songs throughout his career, including \"Wicked Game\" and \"Somebody's Crying,\" \"Baby Did a Bad Bad Thing\" remains one of his most iconic tracks, cementing his status as a unique voice in American rock music.The song's inclusion in \"Eyes Wide Shut\" introduced it to a broader audience and associated it with the film's themes of desire and betrayal. This cinematic connection has ensured the song's place in popular culture, often being referenced or used in other media since its initial release.Chris Isaak continues to perform the song in his live shows, where it remains a fan favorite, demonstrating its lasting appeal and the enduring quality of Isaak's musicianship.'],'Confidence':0.98}\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m kk\u001b[38;5;241m=\u001b[39ma\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(kk)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkk\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# a= json.loads(str({'New_Question': ['Who created or developed the widely recognized \"Ten Commandments of Computer Ethics\"?']}))\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# a.get('New_Question',None)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/confident/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/miniconda3/envs/confident/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/miniconda3/envs/confident/lib/python3.11/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "a='''{'Answer':['The song \"Baby Did a Bad Bad Thing\" was sung by American singer-songwriter Chris Isaak. It was released in 1995 as part of his sixth studio album \"Forever Blue.\" The song gained significant popularity and wider recognition when it was featured in Stanley Kubrick's 1999 film \"Eyes Wide Shut,\" starring Tom Cruise and Nicole Kidman. Chris Isaak, known for his distinctive voice and retro rock and roll style, wrote and performed this sultry, blues-influenced track. The song's provocative lyrics and Isaak's deep, brooding vocals contributed to its memorable and somewhat controversial nature. It showcases Isaak's ability to blend vintage rock sounds with contemporary production, creating a timeless quality that has helped the song endure.\"Baby Did a Bad Bad Thing\" reached number 27 on the UK Singles Chart and became one of Isaak's most recognizable hits. The music video for the song, directed by Herb Ritts, featured supermodel Helena Christensen and added to the song's sensual and mysterious aura.While Chris Isaak has had several other successful songs throughout his career, including \"Wicked Game\" and \"Somebody's Crying,\" \"Baby Did a Bad Bad Thing\" remains one of his most iconic tracks, cementing his status as a unique voice in American rock music.The song's inclusion in \"Eyes Wide Shut\" introduced it to a broader audience and associated it with the film's themes of desire and betrayal. This cinematic connection has ensured the song's place in popular culture, often being referenced or used in other media since its initial release.Chris Isaak continues to perform the song in his live shows, where it remains a fan favorite, demonstrating its lasting appeal and the enduring quality of Isaak's musicianship.'],'Confidence':0.98}'''\n",
    "\n",
    "k=a.replace(\"}\",\"\").replace(\"{\",\"\").replace(\"\\n\",\"\")\n",
    "kk=a.replace(\"\\n\",\"\")\n",
    "print(kk)\n",
    "print(json.loads(kk))\n",
    "\n",
    "# a= json.loads(str({'New_Question': ['Who created or developed the widely recognized \"Ten Commandments of Computer Ethics\"?']}))\n",
    "# a.get('New_Question',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class anthropic_GPT:\n",
    "    def __init__(self,model,api_key):\n",
    "        self.model_name=model\n",
    "        self.api_key=api_key\n",
    "        self.anthropic_client = anthropic.Anthropic(api_key=self.api_key,)\n",
    "        self.APIValidation=False\n",
    "        self.complete_tokens=0\n",
    "        self.prompt_tokens=0\n",
    "        self.re_gen_times=1\n",
    "\n",
    "    def  claude_reply(self,system_prompt='',Instruction='',question='',input_text='',temperature=0,max_tokens=4096,assit_prompt=\"\"):\n",
    "        if input_text:\n",
    "            for _ in range(self.re_gen_times):\n",
    "                try:\n",
    "                    response = self.anthropic_client.messages.create(\n",
    "                        model=self.model_name,\n",
    "                        max_tokens=1000,\n",
    "                        temperature=0,\n",
    "                        system=f\"{str(system_prompt)}\\n{str(assit_prompt)}\",\n",
    "                        messages=[\n",
    "\n",
    "                            {\n",
    "                                \"role\": \"user\",\n",
    "                                \"content\": [\n",
    "                                    {\"type\": \"text\",\n",
    "                                    \"text\":f\"{str(Instruction)} {str(question)} {str(input_text)}\"\n",
    "                                    }\n",
    "                                            ]\n",
    "                            }\n",
    "                        ]\n",
    "                    )\n",
    "                    claim_text=response.content[0].text\n",
    "                    return claim_text\n",
    "                except:\n",
    "                    return None\n",
    "\n",
    "\n",
    "gpt=anthropic_GPT(\"claude-3-5-sonnet-20240620\",key['claude']['api_key'])\n",
    "\n",
    "\n",
    "system_prompt=f\"This is a Long form generation QA task, provide very long Answer with more details to the question and confidence to the Answer in json.\"\n",
    "\n",
    "Instruction=\"\"\n",
    "\n",
    "question='What is the capital of India?'\n",
    "\n",
    "vanilla_prompt=f'''\\nOnly give me the reply according to response format, don't give me any other words.\\n\\nresponse format :\\nAnswer: [Your final Answer here],\\nConfidence : [Your final Confidence here]\\n'''\n",
    "\n",
    "confidence_define_prompt=\"Note: The confidence indicates how likely you think your Answer is true and correct,from 0.00 (worst) to 1.00 (best)\"\n",
    "\n",
    "prompt={\"system_prompt\":system_prompt,'Instruction':Instruction,\"question\":f\"Question : {question}\",'input_text':vanilla_prompt,\"assit_prompt\":confidence_define_prompt}\n",
    "\n",
    "\n",
    "print(prompt)\n",
    "result=gpt.claude_reply(**prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "k=json.loads(result)\n",
    "print(k)\n",
    "ans=k.get(\"Answer\",None)\n",
    "conf=k.get(\"Confidence\",None)\n",
    "explain=k.get(\"Explanation\",None)\n",
    "\n",
    "print(ans,conf,explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset=load_dataset(\"din0s/asqa\")['train']\n",
    "qa_pairs=\"\"\n",
    "print(dataset)\n",
    "for idx,i in enumerate(dataset):\n",
    "    qa_pairs+=f\"Q:{i['ambiguous_question']}\\nA:{i['annotations'][0]['long_answer']}\\n\\n\"\n",
    "    if idx==8:\n",
    "        break\n",
    "with open(\"standard_base_asqa.txt\",'w') as f:\n",
    "    f.writelines(qa_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import randint\n",
    "with open('CAPR/exmaple.json','r') as f:\n",
    "    data=json.load(f)\n",
    "\n",
    "show_index=randint(0,len(data)-1)\n",
    "for k,v in data[show_index].items():\n",
    "    print(k)\n",
    "    print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import json\n",
    "def Get_auroc(accuracy,confidence_scores):\n",
    "    y_true=np.where(np.array(accuracy) < 0.6,0,1)\n",
    "    return roc_auc_score(np.array(y_true), np.array(confidence_scores))\n",
    "\n",
    "with open('CAPR/din0s_asqa_r4_with_vanilla_Vanilla.json','r') as f:\n",
    "    data=json.load(f)\n",
    "\n",
    "print(Get_auroc(data['acc'],data['conf']))\n",
    "print(Get_auroc(data['acc_capr'],data['conf_capr']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "with open('CAPR/PPO_State_06122032_vanilla_f1_r11/reward.json','r') as f:\n",
    "    data=json.load(f)\n",
    "movin_avg=sum(data[:5])/5\n",
    "k1=[]\n",
    "for i in data:\n",
    "    movin_avg=0.99*movin_avg+(1-0.99)*i\n",
    "    k1.append(movin_avg)\n",
    "\n",
    "plt.plot(range(len(k1)),k1,label='reward')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "with open('CAPR/PPO_State_06122032_vanilla_f1_r11/Accuracy.json','r') as f:\n",
    "    data=json.load(f)\n",
    "movin_avg=sum(data[:5])/5\n",
    "k2=[]\n",
    "for i in data:\n",
    "    movin_avg=0.99*movin_avg+(1-0.99)*i\n",
    "    k2.append(movin_avg)\n",
    "{'New_Question': ['When was the king size bed first introduced to the market?']}\n",
    "plt.plot(range(len(k2)),k2,label='accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "with open('CAPR/PPO_State_06122032_vanilla_f1_r11/ECE.json','r') as f:\n",
    "    data=json.load(f)\n",
    "movin_avg=sum(data[:5])/5\n",
    "k2=[]\n",
    "for i in data:\n",
    "    movin_avg=0.99*movin_avg+(1-0.99)*i\n",
    "    k2.append(movin_avg)\n",
    "\n",
    "plt.plot(range(len(k2)),k2,label='ECE')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from netcal.metrics import ECE\n",
    "from torch import rand\n",
    "import numpy as np{'New_Question': ['When was the king size bed first introduced to the market?']}\n",
    "def get_ece(y_confs,y_true):\n",
    "    y_confs=np.array([i.item() for i in y_confs])\n",
    "    y_true=np.array([i.item() for i in y_true])\n",
    "    accuracy = np.mean(y_true)\n",
    "    # y_true=np.where(y_true < accuracy,0,1) ## change to binary ## init 0.59\n",
    "    # ECE\n",
    "    n_bins = 10\n",
    "    # diagram = ReliabilityDiagram(n_bins)\n",
    "    ece = ECE(n_bins)\n",
    "    ece_score = ece.measure(y_confs, y_true)\n",
    "    # print(\"ECE:\", ece_score)\n",
    "    return torch.tensor(ece_score)\n",
    "\n",
    "y_true=rand(size=(128,))/10+0.5\n",
    "y_conf=rand(size=(128,))/10+0.8\n",
    "\n",
    "ece=torch.abs(y_true-y_conf)\n",
    "print(-torch.mean(ece))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import evaluate\n",
    "def calculate_rouge(reference, hypothesis):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, hypothesis)\n",
    "    print(scores)\n",
    "    return scores['rougeL'].fmeasure\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "reference = [\"The cat sat on the mat.\"]\n",
    "hypothesis = [\"The cat sat in the rain\"]\n",
    "scores = [calculate_rouge(i,j) for i,j in zip(reference, hypothesis)]\n",
    "print(scores)\n",
    "\n",
    "\n",
    "# rouge=evaluate.load('rouge')\n",
    "\n",
    "# rouge_results=[rouge.compute(predictiopni,j) for i,j in zip(hypothesis,reference)]\n",
    "# print(rouge_results)\n",
    "# print(rouge_results['rougeL'])\n",
    "# print((rouge_results['rougeL'] > 0.3).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json,random,os\n",
    "\n",
    "# datapath=\"response_result/20240517/din0s_asqa_gpt-3.5-turbo-0125_cot_Long_QA_gpt-3.5-turbo-0125_bertscore.json\"\n",
    "\n",
    "def get_datapath(dataset,api_model,activate_time,acc_model,sim_model,prompt_strategy,isshuffle_str)-> list:\n",
    "\n",
    "\n",
    "    task=[\"QA\",\"Long_QA\"]\n",
    "    datapath=[]\n",
    "    for i in prompt_strategy:\n",
    "        for t in task:\n",
    "            path=f\"response_result/{activate_time}/{dataset}_{api_model}_{i}_{t}_{sim_model}_{acc_model}_{isshuffle_str}.json\"\n",
    "            print(path)\n",
    "            if os.path.isfile(path):\n",
    "                datapath.append(path)\n",
    "\n",
    "    return datapath\n",
    "\n",
    "def mean(data):\n",
    "    return sum(data)/len(data)\n",
    "\n",
    "def Load_data(datapath):\n",
    "    with open(datapath,'r') as f:\n",
    "        data=json.load(f)\n",
    "\n",
    "    conf=[i['Confidence'] for i in data]\n",
    "    ## MAX\n",
    "    simi=[max(map(float,i['Doc_Ans_simi'])) for i in data]\n",
    "    ## Mean\n",
    "    mean_simi=[sum(map(float,i['Doc_Ans_simi']))/len(i['Doc_Ans_simi']) for i in data]\n",
    "    acc=[i['Accuracy'] for i in data]\n",
    "    assert len(simi)==len(conf)\n",
    "    return [conf,mean_simi,acc]\n",
    "\n",
    "# color=['lightblue','lightred',\"lightgreen\",\"yellow\",'pink','lightbrown']\n",
    "def Get_histogram(datalist,dataset,title,stretagy):\n",
    "# Generate sample data\n",
    "    # data1 = np.random.normal(0, 1, 1000)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    colors=plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    # assert len(lable)==len(datalist)\n",
    "    # Plot histograms\n",
    "    for idx,i in enumerate(datalist):\n",
    "        plt.hist(i, bins=100, alpha=0.7, label=stretagy[idx],color=colors[idx% len(colors)])\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f\"{dataset}_{title}\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlim([0, 1])\n",
    "    # plt.ylim([0, 600])\n",
    "    # Show the plot\n",
    "    plt.savefig(f\"picture/histogram/{dataset}_{title}.png\")\n",
    "    # plt.show()\n",
    "    plt.clf\n",
    "\n",
    "def convert_binary_acc(y_true):\n",
    "    mean_acc=[np.mean(np.array(i)) for i in y_true] ## change to binary\n",
    "\n",
    "    y_true=[np.where(np.array(i) < mean_acc[idx],0,1) for idx,i in enumerate(y_true)] ## change to binary\n",
    "    return y_true\n",
    "\n",
    "\n",
    "def show_histogram_graph(vector,title,File_name,stretagy=\"\",sim=\"\",datafile_name=\"\",label=[]):\n",
    "    os.makedirs(f\"PACE/picture/histogram/{File_name}\",exist_ok=True)\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    colors=plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    random.shuffle(colors)\n",
    "    for i,j in zip(vector,label):\n",
    "        plt.hist(i, bins=100, density=True, alpha=0.7, color=colors[random.randint(0,len(colors)-1)], edgecolor='black',label=j)\n",
    "    # Add a title and labels\n",
    "    plt.title(f'{title}')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    # plt.ylim(0,1)\n",
    "    plt.xlim(0,1)\n",
    "    # Add a grid\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='upper right')\n",
    "    # Show plot\n",
    "    plt.savefig(f\"PACE/picture/histogram/{File_name}/{datafile_name}.png\")\n",
    "    # plt.show()\n",
    "    plt.clf\n",
    "\n",
    "def show_plot_graph(vector,title,File_name,stretagy=\"\",sim=\"\",datafile_name=\"\",label=\"\"):\n",
    "    os.makedirs(f\"PACE/picture/histogram/{File_name}\",exist_ok=True)\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    colors=plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    random.shuffle(colors)\n",
    "    # vector=sorted(vector)\n",
    "    counts, bin_edges = np.histogram(vector, bins=len(vector)//10)\n",
    "\n",
    "    # 計算每個柱的中心點\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    # plt.hist(vector, bins=30, alpha=0.3, color='gray', edgecolor='black')\n",
    "    plt.plot(bin_centers,counts, marker='.',linestyle='-', color=colors[random.randint(0,len(colors)-1)],label=label)\n",
    "    # Add a title and labels\n",
    "    plt.title(f'{title}')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    # plt.ylim(0,1)\n",
    "    # Add a grid\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='upper right')\n",
    "    # Show plot\n",
    "    plt.savefig(f\"PACE/picture/histogram/{File_name}/{datafile_name}.png\")\n",
    "    # plt.show()\n",
    "    plt.clf\n",
    "\n",
    "\n",
    "def Update_Fig(activate_time,shuffle):\n",
    "    isshuffle_str=\"shuffle\" if shuffle else \"No_shuffle\"\n",
    "    datapaht=f\"./PACE/response_result/Evaluate_Result_{activate_time}_{isshuffle_str}.json\"\n",
    "    with open(datapaht,'r') as f:\n",
    "        data=json.load(f)\n",
    "\n",
    "    os.makedirs(\"PACE/picture/histogram\",exist_ok=True)\n",
    "    simi_models=[\"Cos_sim\"]\n",
    "    datasets=[\"din0s/asqa\"]\n",
    "    api_model='gpt-3.5-turbo-0125'\n",
    "    acc_model='rougeL'\n",
    "\n",
    "    stretagy=[\"vanilla\",'cot',\"multi_step\"]\n",
    "\n",
    "    for sim in simi_models:\n",
    "        for dataset in datasets:\n",
    "            for dd in data:\n",
    "                for k in stretagy:\n",
    "                    # conf_list,Final_conf_list,simi_list,acc_list=[],[],[],[]\n",
    "                    if dd['dataset']==dataset and dd['sim_model']==sim and dd['Stratagy']==k and dd['acc_model']==acc_model:\n",
    "                        print(f\"Load Sucess {dataset} {k} {sim}\")\n",
    "                        print(f\"Accuracy {np.mean(np.array(dd['Accuracy']))}\")\n",
    "                        print(f\"ECE {np.mean(np.array(dd['ece']))}\")\n",
    "                        print(f\"AUROC {np.mean(np.array(dd['auroc']))}\")\n",
    "                        dataset_path=dataset.replace(\"/\",\"_\")\n",
    "                        print(dd['Simi'])\n",
    "                        show_plot_graph(dd['Conf'],File_name=f\"{activate_time}_{isshuffle_str}\",title=f\"{k} {sim} Confidence\",stretagy=k,sim=f\"{sim}\",datafile_name=f\"{dataset_path}_{isshuffle_str}_{sim}_{k}_Confidence\",label=f\"{isshuffle_str}\")\n",
    "\n",
    "                        show_plot_graph(dd['Simi'],File_name=f\"{activate_time}_{isshuffle_str}\",title=f\"{k} {sim} Similarity\",stretagy=k,sim=f\"{sim}\",datafile_name=f\"{dataset_path}_{isshuffle_str}_{sim}_{k}_Similarity\",label=f\"{isshuffle_str}\")\n",
    "\n",
    "                        show_plot_graph(dd['Pace_Conf'],File_name=f\"{activate_time}_{isshuffle_str}\",title=f\"{k} {sim} Final Confidence\",stretagy=k,sim=f\"{sim}\",datafile_name=f\"{dataset_path}_{isshuffle_str}_{sim}_{k}_PACE_Confidence\",label=f\"{isshuffle_str}\")\n",
    "\n",
    "                        show_plot_graph(dd['Accuracy'],File_name=f\"{activate_time}_{isshuffle_str}\",title=f\"{k} {sim} Accuracy\",stretagy=k,sim=f\"{sim}\",datafile_name=f\"{dataset_path}_{isshuffle_str}_{sim}_{k}_Accuracy\",label=f\"{isshuffle_str}\")\n",
    "\n",
    "Update_Fig(\"20240601\",False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json,random,os\n",
    "\n",
    "def show_four_plot(vectorlist,title):\n",
    "    colors=plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    random.shuffle(colors)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    fig.patch.set_facecolor('lightgrey')\n",
    "    # Plot each vector list in its respective subplot\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        counts, bin_edges = np.histogram(vectorlist[i], bins=len(vectorlist[i])//10)\n",
    "        # 計算每個柱的中心點\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        # plt.hist(vector, bins=30, alpha=0.3, color='gray', edgecolor='black')\n",
    "\n",
    "        ax.plot(bin_centers,counts, marker='.',linestyle='-', color=colors[random.randint(0,len(colors)-1)],label='No Shuffle')\n",
    "        # ax.set_xlabel('Index')\n",
    "        if i%2:\n",
    "            ax.set_ylabel('Accuracy',fontsize=12)\n",
    "        else:\n",
    "            ax.set_ylabel('Confidence',fontsize=12)\n",
    "        ax.set_xlim((0,1))\n",
    "        ax.set_facecolor('whitesmoke')\n",
    "    # fig.suptitle(title, fontsize=16)\n",
    "    # Adjust layout to prevent overlapping\n",
    "    plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def Update_Fig(activate_time,shuffle):\n",
    "    isshuffle_str=\"shuffle\" if shuffle else \"No_shuffle\"\n",
    "    datapaht=f\"./PACE/response_result/Evaluate_Result_{activate_time}_{isshuffle_str}.json\"\n",
    "    with open(datapaht,'r') as f:\n",
    "        data=json.load(f)\n",
    "\n",
    "    os.makedirs(\"PACE/picture/histogram\",exist_ok=True)\n",
    "    simi_models=[\"Cos_sim\"]\n",
    "    datasets=[\"din0s/asqa\"]\n",
    "    api_model='gpt-3.5-turbo-0125'\n",
    "    acc_model='rougeL'\n",
    "\n",
    "    stretagy=[\"vanilla\",'cot',\"multi_step\"]\n",
    "    vector=[]\n",
    "    for sim in simi_models:\n",
    "        for dataset in datasets:\n",
    "            for dd in data:\n",
    "                for k in stretagy:\n",
    "                    # conf_list,Final_conf_list,simi_list,acc_list=[],[],[],[]\n",
    "                    if dd['dataset']==dataset and dd['sim_model']==sim and dd['Stratagy']==k and dd['acc_model']==acc_model:\n",
    "                        vector+=[dd['Conf'],dd['Accuracy']]\n",
    "                        show_four_plot([dd['Conf'],dd['Accuracy']],k)\n",
    "\n",
    "Update_Fig(20240601,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_histogram_graph(vector,dim_x_y,stretagy=\"\"):\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.hist(vector, bins=30, density=True, alpha=0.7, color='blue', edgecolor='black')\n",
    "    # Add a title and labels\n",
    "    plt.title(f'{stretagy} Score')\n",
    "    plt.xlabel('Confidence Value')\n",
    "    plt.ylabel('Density')\n",
    "    # plt.ylim(0,1)\n",
    "    plt.xlim(0,1)\n",
    "    # Add a grid\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show plot\n",
    "    # plt.savefig(f\"picture/histogram/{dataset}_{title}.png\")\n",
    "    plt.show()\n",
    "    plt.clf\n",
    "\n",
    "from netcal.metrics import ECE\n",
    "\n",
    "activate_time=\"20240529\"\n",
    "\n",
    "os.makedirs(\"picture/histogram\",exist_ok=True)\n",
    "simi_models=[\"Cos_sim\",\"snli\"]\n",
    "datasets=[\"natural_questions\",'din0s_asqa']\n",
    "api_model='gpt-3.5-turbo-0125'\n",
    "acc_model='bertscore'\n",
    "\n",
    "stretagy=[\"vanilla\",'cot',\"multi_step\"]\n",
    "\n",
    "simi_model=\"snli\"\n",
    "dataset=\"din0s_asqa\"\n",
    "\n",
    "path_list=get_datapath(dataset=dataset,api_model=api_model,activate_time=activate_time,acc_model=acc_model,sim_model=simi_model,prompt_strategy=stretagy)\n",
    "\n",
    "evaldata=list(map(Load_data,path_list))\n",
    "\n",
    "for k in range(3):\n",
    "    accuracy=np.mean(np.array(evaldata[k][2]))\n",
    "    y_confs=evaldata[k][0]\n",
    "    y_true=list(map(float,evaldata[k][2]))\n",
    "    y_true=np.where(np.array(y_true) < 0.9,0,1)\n",
    "\n",
    "    ll=0.5\n",
    "    pace_conf_array = np.add(ll*np.array(evaldata[k][0]),(1-ll)*np.array(evaldata[k][1]))\n",
    "\n",
    "    # ECE\n",
    "    n_bins = 10\n",
    "    # diagram = ReliabilityDiagram(n_bins)\n",
    "    ece = ECE(n_bins)\n",
    "    assert len(y_confs)==len(y_true)\n",
    "    ece_score = ece.measure(np.array(y_confs), np.array(y_true),uncertainty='mean')\n",
    "    print(\"ECE:\", ece_score)\n",
    "\n",
    "    n_bins = 10\n",
    "    # diagram = ReliabilityDiagram(n_bins)\n",
    "    ece = ECE(n_bins)\n",
    "    ece_pace_score = ece.measure(pace_conf_array, np.array(y_true))\n",
    "    print(\"ECE_PACE:\", ece_pace_score)\n",
    "\n",
    "    print(f\"conf origin {np.mean(y_confs)}, PACE {np.mean(pace_conf_array)}\")\n",
    "\n",
    "    show_histogram_graph(y_confs,dim_x_y=[0,1],stretagy=f\"{simi_model}\")\n",
    "\n",
    "    show_histogram_graph(evaldata[k][1],dim_x_y=[0,1],stretagy=f\"{simi_model}\")\n",
    "\n",
    "    show_histogram_graph(pace_conf_array,dim_x_y=[0,1],stretagy=f\"{simi_model}\")\n",
    "\n",
    "    # show_histogram_graph(y_true,dim_x_y=[0,1],stretagy=f\"{simi_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "def split_text_into_fixed_length_parts(text, tokens_per_part, model_name='bert'):\n",
    "\n",
    "    model_huggingface={\n",
    "        'bert':'bert-base-uncased',\n",
    "        'xbert':'efederici/sentence-bert-base',\n",
    "    }\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_huggingface[model_name])\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Initialize the list to hold each part\n",
    "    parts = []\n",
    "\n",
    "    # Calculate the number of full parts\n",
    "    full_parts = len(tokens) // tokens_per_part\n",
    "\n",
    "    # Create parts with exactly tokens_per_part tokens\n",
    "    for i in range(full_parts):\n",
    "        start_index = i * tokens_per_part\n",
    "        end_index = start_index + tokens_per_part\n",
    "        part_tokens = tokens[start_index:end_index]\n",
    "        # Convert token list to string and add to the parts list\n",
    "        parts.append(tokenizer.convert_tokens_to_string(part_tokens))\n",
    "\n",
    "    # Handle the remaining tokens, if any\n",
    "    if len(tokens) % tokens_per_part:\n",
    "        remaining_tokens = tokens[full_parts * tokens_per_part:]\n",
    "        parts.append(tokenizer.convert_tokens_to_string(remaining_tokens))\n",
    "\n",
    "    return parts\n",
    "\n",
    "# Example usage\n",
    "text = \"Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.\"\n",
    "tokens_per_part = 96\n",
    "result = split_text_into_fixed_length_parts(text, tokens_per_part,'bert')\n",
    "result = split_text_into_fixed_length_parts(text, tokens_per_part,'xbert')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "from evaluate import load\n",
    "def load_data():\n",
    "    with open('/home/thesis/confidence_Score/response_result/gpt-3.5-turbo-0125_vanilla_simi_2024_05_08.json','r') as f:\n",
    "        data=json.load(f)\n",
    "    # print(data.keys())\n",
    "    simi_res=data['When did the kokoda war start and end?']['similarity_res']\n",
    "    olddat=simi_res[0]\n",
    "    for i in simi_res:\n",
    "        if i[1] > olddat[1]:\n",
    "            olddat=i\n",
    "    ans,long_ans=olddat[2],olddat[3]\n",
    "    return ans,long_ans\n",
    "def Bernormalize(ans,long_ans):\n",
    "    nomalizer=BertNormalizer(clean_text=True,lowercase=True,handle_chinese_chars=True)\n",
    "    return nomalizer.normalize_str(ans),nomalizer.normalize_str(long_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WER metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans,long_ans=load_data()\n",
    "print(ans)\n",
    "print(long_ans)\n",
    "wer_metric = load(\"wer\")\n",
    "acc_wer = wer_metric.compute(references=[ans], predictions=[long_ans])\n",
    "print(f\"WER acc : {acc_wer}\")\n",
    "print(f\"1- WER acc : {1-acc_wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans,long_ans=load_data()\n",
    "print(ans)\n",
    "print(long_ans)\n",
    "exact_match_metric = load(\"exact_match\")\n",
    "results = exact_match_metric.compute(predictions=[ans], references=[long_ans])\n",
    "print(results['exact_match'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "ans,long_ans=load_data()\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embedding_1= model.encode(ans, convert_to_tensor=True)\n",
    "embedding_2 = model.encode(long_ans, convert_to_tensor=True)\n",
    "\n",
    "result=util.pytorch_cos_sim(embedding_1, embedding_2)\n",
    "\n",
    "print(result.item())\n",
    "## tensor([[0.6003]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "from evaluate import load\n",
    "ans,long_ans=load_data()\n",
    "print(ans)\n",
    "print(long_ans)\n",
    "\n",
    "\n",
    "P, R, F1 = score([ans], [long_ans],lang=\"en\",verbose=True)\n",
    "# Print scores\n",
    "print(\"Precision: \", P.item())\n",
    "print(\"Recall: \", R.item())\n",
    "print(\"F1 Score: \", F1.item())\n",
    "\n",
    "\n",
    "bertscore = load(\"bertscore\")\n",
    "result=bertscore.compute(predictions=[ans],references=[long_ans],lang='en',verbose=True)\n",
    "print(\"Precision: \", result['precision'].pop())\n",
    "print(\"Recall: \", result['recall'].pop())\n",
    "print(\"F1 Score: \", result['f1'].pop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import load_checkpoint\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "datapath=f'response_result/gpt-3.5-turbo-0125_vanilla_2024_05_09.json'\n",
    "datadict=load_checkpoint(datapath)\n",
    "\n",
    "class eval_dataloader:\n",
    "    def __init__(self,dataset_path,batch_size=1,purpose='eval') -> None:\n",
    "        self.dataset = load_checkpoint(dataset_path)\n",
    "        if purpose==\"eval\":\n",
    "            self.loader=DataLoader(list(self.dataset.values()),batch_size=batch_size,collate_fn=self.simi_acc_collate_fn,shuffle=True)\n",
    "        elif purpose==\"ece\":\n",
    "            self.loader=DataLoader(list(self.dataset.values()),batch_size=batch_size,collate_fn=self.ece_collate_fn,shuffle=False)\n",
    "\n",
    "    def simi_acc_collate_fn(self,batch):\n",
    "        res=[]\n",
    "        for i in batch:\n",
    "            res.append([i['Question'],i['Document'],i['Answer'],i['Long Answer'],i['Confidence']])\n",
    "        return res\n",
    "\n",
    "    def ece_collate_fn(self,batch):\n",
    "        simi_res=[]\n",
    "        conf_res=[]\n",
    "        accres=[]\n",
    "        for i in batch:\n",
    "            simi_res.append(i['similarity_res'])\n",
    "            conf_res.append(i['confidence'])\n",
    "            accres.append(i['acc'])\n",
    "        return simi_res,conf_res,accres\n",
    "\n",
    "\n",
    "def conf_calibration(simi,conf):\n",
    "    x_lambda=0.5\n",
    "    return x_lambda*simi+(1-x_lambda)*conf\n",
    "\n",
    "def ece_calibration(simi:list,acc:list,conf:list): # batch b_m\n",
    "    conf=list(map(conf_calibration,simi,conf))\n",
    "    assert len(acc)==len(simi)\n",
    "    b_m=len(acc)\n",
    "    ece=np.mean(np.array(acc)-np.array(conf))/b_m\n",
    "    return ece\n",
    "\n",
    "def get_most_high_simi(simi_res:list):\n",
    "    return sorted(simi_res,key=lambda x:x[1],reverse=True)[0][1]\n",
    "\n",
    "eve_l=eval_dataloader(datapath,2,'eval').loader\n",
    "print(len(eve_l))\n",
    "# for simi_res,conf_res,accres in eve_l:\n",
    "    # most_simi=list(map(get_most_high_simi,simi_res))\n",
    "    # print(ece_calibration(most_simi,accres,conf_res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "\n",
    "def compute_macro_f1(sentence1, sentence2):\n",
    "    # Tokenize the sentences\n",
    "    tokens1 = sentence1.split()\n",
    "    tokens2 = sentence2.split()\n",
    "\n",
    "    # Count the frequency of each token in both sentences\n",
    "    counter1 = Counter(tokens1)\n",
    "    counter2 = Counter(tokens2)\n",
    "\n",
    "    # Create a union of all unique tokens\n",
    "    all_tokens = list(set(tokens1) | set(tokens2))\n",
    "\n",
    "    # Create binary vectors for comparison\n",
    "    vector1 = torch.tensor([counter1[token] for token in all_tokens])\n",
    "    vector2 = torch.tensor([counter2[token] for token in all_tokens])\n",
    "\n",
    "    # Compute the macro-F1 score\n",
    "    f1 = torch.tensor(f1_score(vector1.numpy(), vector2.numpy(), average='macro'),dtype=torch.float16)\n",
    "\n",
    "    return f1\n",
    "\n",
    "# Example sentences\n",
    "sentence1 = \"The quick brown fox jumps over the lazy dog\"\n",
    "sentence2 = \"A quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "macro_f1 = compute_macro_f1(sentence1, sentence2)\n",
    "print(\"Macro F1 Score:\", macro_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Money Spent on API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob\n",
    "from util import load_checkpoint\n",
    "def Calulate_spent(file_path='response_result/gpt-3.5-turbo-0125',api_model=\"gpt-3.5-turbo-0125\"):\n",
    "    compete_toekn=0\n",
    "    prompt_token=0\n",
    "\n",
    "    for datapath in glob.glob(f'{file_path}*.json'):\n",
    "        if os.path.isfile(datapath):\n",
    "            datares=load_checkpoint(datapath)\n",
    "            print(f\"{datapath} {len(datares)}\")\n",
    "            for i in datares.values():\n",
    "                if \"Complete_tokens\" in i:\n",
    "                    compete_toekn+=i['Complete_tokens']\n",
    "                if \"Prompt_tokens\" in i:\n",
    "                    prompt_token+=i['Prompt_tokens']\n",
    "\n",
    "    Total_Spent =compete_toekn*8.00/1000000+prompt_token*6.00/1000000\n",
    "    print(f\"Complete tokens :{compete_toekn},prompt_tokens :{prompt_token} \")\n",
    "    print(f\"Total Spent {Total_Spent} USD ; {Total_Spent*30} TWD\")\n",
    "\n",
    "\n",
    "Calulate_spent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "def is_answer_correct(question, true_answer, predicted_answer):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Get embeddings\n",
    "    true_embedding = get_embedding(true_answer, model, tokenizer)\n",
    "    predicted_embedding = get_embedding(predicted_answer, model, tokenizer)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(true_embedding.numpy(), predicted_embedding.numpy())[0][0]\n",
    "\n",
    "    # Threshold for correctness\n",
    "    threshold = 0.8\n",
    "    print(similarity)\n",
    "    return similarity >= threshold\n",
    "\n",
    "# Example question and answers\n",
    "question = \"What is the capital of France?\"\n",
    "true_answer = \"The capital of France is Paris.\"\n",
    "predicted_answer = \"Paris is the capital of France.\"\n",
    "\n",
    "is_correct = is_answer_correct(question, true_answer, predicted_answer)\n",
    "print(\"Is the answer correct?\", is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    # Simple sentence tokenizer (could be improved with more sophisticated methods)\n",
    "    return text.split('. ')\n",
    "\n",
    "def evaluate_long_form_qa(true_answer, predicted_answer):\n",
    "    # Tokenize the answers into sentences\n",
    "    true_sentences = tokenize_sentences(true_answer)\n",
    "    predicted_sentences = tokenize_sentences(predicted_answer)\n",
    "\n",
    "    # Generate binary labels\n",
    "    true_labels = [1] * len(true_sentences)  # Assume all sentences in the ground truth are correct\n",
    "    predicted_labels = [1 if sentence in true_sentences else 0 for sentence in predicted_sentences]\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Example long-form answers\n",
    "true_answer = \"The quick brown fox jumps over the lazy dog. Foxes are known for their agility. Dogs can be quite lazy at times.\"\n",
    "predicted_answer = \"The quick brown fox jumps over the lazy dog. Foxes are very agile animals. Sometimes dogs are lazy.\"\n",
    "\n",
    "precision, recall, f1 = evaluate_long_form_qa(true_answer, predicted_answer)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "eval_model = BERTScorer(\n",
    "                        model_type=\"bert-base-uncased\",  # Model type\n",
    "                        num_layers=9,  # Number of layers to use\n",
    "                        all_layers=False,  # Whether to use all layers\n",
    "                        idf=False,  # Whether to use IDF scaling\n",
    "                        batch_size=64,  # Batch size\n",
    "                        lang=None,  # Language of the texts, auto-detect based on model if None\n",
    "                        rescale_with_baseline=False,  # Whether to rescale\n",
    "                        device='cuda:1'\n",
    "                    )\n",
    "def tokenize_sentences(text):\n",
    "    # Simple sentence tokenizer (could be improved with more sophisticated methods)\n",
    "    return text.split('. ')\n",
    "true_answer = \"The quick brown fox jumps over the lazy dog. Foxes are known for their agility. Dogs can be quite lazy at times.\"\n",
    "predicted_answer = \"The quick brown fox. The quick brown fox. The quick brown fox.\"\n",
    "\n",
    "true_sentences=tokenize_sentences(true_answer)\n",
    "predicted_sentences=tokenize_sentences(predicted_answer)\n",
    "true_labels = [1] * len(true_sentences)  # Assume all sentences in the ground truth are correct\n",
    "predicted_labels = [1 if sentence in true_sentences else 0 for sentence in predicted_sentences]\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "conf_path='response_result/gpt-3.5-turbo-0125_vanilla_2024_05_09.json'\n",
    "simi_acc_path='response_result/gpt-3.5-turbo-0125_vanilla_simi_acc_2024_05_09.json'\n",
    "with open(simi_acc_path,'r') as f:\n",
    "    data1=json.load(f)\n",
    "    print(len(data1))\n",
    "    for i in data1.keys():\n",
    "        print(i)\n",
    "\n",
    "with open(conf_path,'r') as f:\n",
    "    data2=json.load(f)\n",
    "    print(len(data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def show(conf_list):\n",
    "    if isinstance(conf_list,dict):\n",
    "        print(f\"Evaluate Result:\")\n",
    "        for v in conf_list.values():\n",
    "            print(f\"      {v['Stratagy']}:\")\n",
    "            for k1,v1 in v.items():\n",
    "                print(f\"        {k1} : {v1}\")\n",
    "    elif isinstance(conf_list,list):\n",
    "        print(f\"Evaluate Result:\")\n",
    "        for v in conf_list:\n",
    "            print(f\"      {v['Stratagy']}:\")\n",
    "            for k1,v1 in v.items():\n",
    "                print(f\"        {k1} : {v1}\")\n",
    "\n",
    "for i in glob.glob(\"response_result/*_eval_*.json\"):\n",
    "    with open(i,'r') as f:\n",
    "        show(json.load(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a =[torch.tensor(i/100) for i in range(0,100)]\n",
    "print(a)\n",
    "print(torch.stack(a,dim=0))\n",
    "# print(torch.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "confidence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
