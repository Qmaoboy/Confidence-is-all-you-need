{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.6433, 0.5027, 0.4554, 0.0000,\n",
      "        0.0000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "acc=torch.tensor([1.0]*5+[0.5]*3+[0.0]*2)\n",
    "conf=torch.rand(10)\n",
    "conf[acc==1.]=1.0\n",
    "conf[acc==0.]=0.0\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import concurrent\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import os,json,re,yaml\n",
    "import textgrad as tg\n",
    "from textgrad.tasks import load_task\n",
    "import numpy as np\n",
    "import random,json\n",
    "\n",
    "question_str=\"Which poisonous compound was used by a religous cult in deadly attacks on the Tokyo subway in 1995?\"\n",
    "answer_str=\"sarin gas\"\n",
    "\n",
    "class text_grad:\n",
    "    def __init__(self) -> None:\n",
    "        load_dotenv(override=True)\n",
    "        llm_api_eval = tg.get_engine(engine_name=\"gpt-4o\")\n",
    "        llm_api_test = tg.get_engine(engine_name=\"gpt-3.5-turbo-0125\")\n",
    "        tg.set_backward_engine(llm_api_eval, override=True)\n",
    "        train_set, val_set, test_set, self.eval_fn = load_task(\"BBH_word_sorting\", evaluation_api=llm_api_eval)\n",
    "\n",
    "        self.STARTING_SYSTEM_PROMPT=\"You will answer a reasoning question with confidence score. Think step by step. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is the answer to the question\"\n",
    "        self.set_up_keys()\n",
    "\n",
    "        self.system_prompt_eval = tg.Variable(self.STARTING_SYSTEM_PROMPT,\n",
    "                            requires_grad=True,\n",
    "                            role_description=\"system prompt to the language model\")\n",
    "        self.model_evaluation = tg.BlackboxLLM(llm_api_eval, self.system_prompt_eval)\n",
    "\n",
    "        self.system_prompt = tg.Variable(self.STARTING_SYSTEM_PROMPT,\n",
    "                                    requires_grad=True,\n",
    "                                    role_description=\"structured system prompt to a somewhat capable language model that specifies the behavior and strategies for the QA task\")\n",
    "        self.model = tg.BlackboxLLM(llm_api_test, self.system_prompt)\n",
    "\n",
    "        self.optimizer = tg.TextualGradientDescent(engine=llm_api_eval, parameters=[self.system_prompt])\n",
    "\n",
    "    def set_up_keys(self):\n",
    "        if os.path.isfile(\"api_key.yml\"):\n",
    "            with open(\"api_key.yml\",\"r\") as f:\n",
    "                key=yaml.safe_load(f)\n",
    "\n",
    "        os.environ['OPENAI_API_KEY'] = key['openai']['api_key']\n",
    "\n",
    "    def generate(self,question_str,answer_str):\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        question = tg.Variable(question_str, role_description=\"question to the LLM\", requires_grad=False)\n",
    "\n",
    "        answer = tg.Variable(answer_str, role_description=\"answer to the question\", requires_grad=False)\n",
    "        prediction_origin = model(question)\n",
    "        print(prediction_origin.__repr__())\n",
    "        loss = self.eval_fn(inputs=dict(prediction=prediction_origin, ground_truth_answer=answer))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        prediction_after = model(question)\n",
    "        print(prediction_after.__repr__())\n",
    "        result={\n",
    "            \"prompt_before\":self.STARTING_SYSTEM_PROMPT,\n",
    "            'prompt_after':self.system_prompt.value,\n",
    "            'answer_before':prediction_origin.value,\n",
    "            # 'grad_fn_before':prediction_origin.get_grad_fn(),\n",
    "            'answer_after':prediction_after.value,\n",
    "            \"ground_truth\":answer.value,\n",
    "            # 'grad_fn_after':prediction_origin.get_grad_fn(),\n",
    "        }\n",
    "        return result\n",
    "\n",
    "tt=text_grad()\n",
    "result=tt.generate(question_str,answer_str)\n",
    "with open('text.json','w') as f:\n",
    "    json.dump(result,f,indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"answer\": \"As of 2023, the best golfer in the world is Jon Rahm, according to the Official World Golf Ranking.\",\n",
      "  \"confidence\": 0.9\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "  \"answer\": \"As of 2023, the best golfer in the world is Jon Rahm, according to the Official World Golf Ranking. Jon Rahm's position is supported by his consistent performance and high ranking in the golf world.\",\n",
      "  \"confidence\": 0.9\n",
      "}\n",
      "```\n",
      "{'Answer': \"As of 2023, the best golfer in the world is Jon Rahm, according to the Official World Golf Ranking. Jon Rahm's position is supported by his consistent performance and high ranking in the golf world.\", 'Confidence': 0.9}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import concurrent\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import os,json,re,yaml\n",
    "import textgrad as tg\n",
    "from textgrad.tasks import load_task\n",
    "import numpy as np\n",
    "import random,json\n",
    "\n",
    "class text_grad:\n",
    "    def __init__(self) -> None:\n",
    "        self.set_up_keys()\n",
    "        tg.set_backward_engine(\"gpt-3.5-turbo-0125\", override=True)\n",
    "\n",
    "        # Step 1: Get an initial response from an LLM.\n",
    "        self.model = tg.BlackboxLLM(\"gpt-4o\")\n",
    "    def set_up_keys(self):\n",
    "        if os.path.isfile(\"api_key.yml\"):\n",
    "            with open(\"api_key.yml\",\"r\") as f:\n",
    "                key=yaml.safe_load(f)\n",
    "\n",
    "        os.environ['OPENAI_API_KEY'] = key['openai']['api_key']\n",
    "\n",
    "    def parser(self,text):\n",
    "        answer_match = re.search(r'\"answer\": \"(.*?)\"', text)\n",
    "        if answer_match:\n",
    "            answer = answer_match.group(1)\n",
    "        # Regex to capture the confidence score\n",
    "        confidence_match = re.search(r'\"confidence\": (\\d+\\.\\d+)', text)\n",
    "        if confidence_match:\n",
    "            confidence_score = float(confidence_match.group(1))\n",
    "\n",
    "        return {\"Answer\":answer,\"Confidence\":confidence_score}\n",
    "\n",
    "    def text_grad_get_response(self,question_str,answer_str):\n",
    "\n",
    "        question_string = (f\"{question_str}\"\n",
    "                        \"provide 'answer' to the question and 'confidence' to the answer in json,confidence is a float value between 0 and 1\")\n",
    "\n",
    "        question = tg.Variable(question_string,\n",
    "                            role_description=\"question to the LLM\",\n",
    "                            requires_grad=False)\n",
    "\n",
    "        answer = self.model(question)\n",
    "\n",
    "        print(answer)\n",
    "        result=self.parser(str(answer))\n",
    "\n",
    "        answer.set_role_description(\"concise and accurate answer to the question\")\n",
    "\n",
    "        # Step 2: Define the loss function and the optimizer, just like in PyTorch!\n",
    "        # Here, we don't have SGD, but we have TGD (Textual Gradient Descent)\n",
    "        # that works with \"textual gradients\".\n",
    "        optimizer = tg.TGD(parameters=[answer])\n",
    "        evaluation_instruction = (f\"Here's a question: {question_string}. \"\n",
    "                                \"Evaluate any given answer to this question, \"\n",
    "                                \"be smart, logical, and very critical. \"\n",
    "                                \"Just provide concise feedback.\"\n",
    "                                )\n",
    "        # TextLoss is a natural-language specified loss function that describes\n",
    "        # how we want to evaluate the reasoning.\n",
    "        loss_fn = tg.TextLoss(evaluation_instruction)\n",
    "\n",
    "        # Step 3: Do the loss computation, backward pass, and update the punchline.\n",
    "        # Exact same syntax as PyTorch!\n",
    "        loss = loss_fn(answer)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(answer)\n",
    "        result=self.parser(answer.value)\n",
    "\n",
    "        return result\n",
    "\n",
    "question_str=\"Who is the best golfer in the world?\"\n",
    "answer_str=\"\"\n",
    "result=text_grad().text_grad_get_response(question_str,answer_str)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import randint\n",
    "class demo:\n",
    "    def __init__(self,path) -> None:\n",
    "        with open(path,\"r\") as f:\n",
    "            self.refine_data=json.load(f)\n",
    "\n",
    "    def Get_original_result(self,Question):\n",
    "        with open('CAPR/response_result/20240601/din0s_asqa_gpt-3.5-turbo-0125_vanilla_Long_QA.json','r') as af:\n",
    "            self.origin_data=json.load(af)\n",
    "\n",
    "        print(self.origin_data[0].keys())\n",
    "\n",
    "        for i in self.origin_data:\n",
    "            if i[\"Question\"]==Question:\n",
    "                self.result['origin_answer']=i[\"Answer\"]\n",
    "                self.result['origin_confidence']=i[\"Confidence\"]\n",
    "                self.result['origin_Instruction']=i['Prompt'][\"Instruction\"]\n",
    "\n",
    "    def Refine_exmple(self,api_model):\n",
    "        temp_acc=0.1\n",
    "        self.result={}\n",
    "        data_size=len(self.refine_data[api_model]['Evaluate_result']['Accuracy'])\n",
    "        print(f\"{api_model} Data size {data_size}\")\n",
    "        idx=randint(0,data_size-1)\n",
    "        for idx in range(data_size):\n",
    "            if self.refine_data[api_model]['Example']['old_prompt'][idx]['Question']==\"Question : When was the first apple i phone made?\":\n",
    "                # print(self.refine_data[api_model]['Example'].keys())\n",
    "                self.result[\"Question\"]=self.refine_data[api_model]['Example']['old_prompt'][idx]['Question']\n",
    "                self.result[\"Origin_Instruction\"]=self.refine_data[api_model]['Example']['old_prompt'][idx]['Instruction']\n",
    "                self.result[\"After_Instruction\"]=self.refine_data[api_model]['Example']['new_prompt'][idx]['Instruction']\n",
    "                self.result[\"Ground_truth\"]=self.refine_data[api_model]['Example']['Ground_truth'][idx]\n",
    "                self.result['origin_answer']=self.refine_data[api_model]['Example']['old_Result'][idx]['Answer']\n",
    "                self.result['origin_Verbal_Confidence']=self.refine_data[api_model]['Example']['old_Result'][idx]['Confidence']\n",
    "                self.result['CAPR_answer']=self.refine_data[api_model]['Example']['Result'][idx]['Answer']\n",
    "                self.result['CAPR_Verbal_Confidence']=self.refine_data[api_model]['Example']['Result'][idx]['Confidence']\n",
    "                self.result['Accuracy']=self.refine_data[api_model]['Evaluate_result']['Accuracy'][idx]\n",
    "                self.result['old_Accuracy']=self.refine_data[api_model]['Evaluate_result']['old_Accuracy'][idx]\n",
    "                self.result['Pace_Conf']=self.refine_data[api_model]['Evaluate_result']['Pace_Conf'][idx]\n",
    "                self.result['old_Pace_Conf']=self.refine_data[api_model]['Evaluate_result']['old_Pace_Conf'][idx]\n",
    "\n",
    "    def retrieve_example(self,api_model):\n",
    "        self.Refine_exmple(api_model)\n",
    "        with open(f\"{api_model}_Example.json\",'w+') as f:\n",
    "            json.dump(self.result,f,indent=4)\n",
    "\n",
    "        # for k,v in self.result.items():\n",
    "            # print(k)\n",
    "            # print(f\"\\t{v}\")\n",
    "\n",
    "dd=demo('CAPR/Inf_din0s_asqa_r12_withPACE.json',)\n",
    "dd.retrieve_example(\"gpt-4-turbo\")\n",
    "dd.retrieve_example(\"gpt-3.5-turbo-0125\")\n",
    "dd.retrieve_example(\"claude-3-5-sonnet-20240620\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moveing Average of RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def Moving_average(data_folder):\n",
    "    data_size=20\n",
    "    ratio=0.99\n",
    "    for k in ['reward','Accuracy','ECE']:\n",
    "        data_path=f'{data_folder}/{k}.json'\n",
    "        if os.path.isfile(data_path):\n",
    "            with open(data_path,'r') as f:\n",
    "                data=json.load(f)\n",
    "            movin_avg=sum(data[:data_size])/data_size\n",
    "            k1=[]\n",
    "            for i in data:\n",
    "                movin_avg=ratio*movin_avg+(1-ratio)*i\n",
    "                k1.append(movin_avg)\n",
    "\n",
    "            plt.plot(range(len(k1)),k1,label=k,marker='')\n",
    "            plt.legend()\n",
    "            plt.savefig(f\"{data_path.replace(\".json\",\"mvavg.png\")}\")\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "\n",
    "Moving_average(\"CAPR/PPO_State_06122032_vanilla_f1_r1_trivia_withPACE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from netcal.metrics import ECE\n",
    "from torch import rand\n",
    "import numpy as np\n",
    "def get_ece(y_confs,y_true):\n",
    "    y_confs=np.array([i.item() for i in y_confs])\n",
    "    y_true=np.array([i.item() for i in y_true])\n",
    "    accuracy = np.mean(y_true)\n",
    "    # y_true=np.where(y_true < accuracy,0,1) ## change to binary ## init 0.59\n",
    "    # ECE\n",
    "    n_bins = 10\n",
    "    # diagram = ReliabilityDiagram(n_bins)\n",
    "    ece = ECE(n_bins)\n",
    "    ece_score = ece.measure(y_confs, y_true)\n",
    "    # print(\"ECE:\", ece_score)\n",
    "    return torch.tensor(ece_score)\n",
    "\n",
    "y_true=rand(size=(128,))/10+0.5\n",
    "y_conf=rand(size=(128,))/10+0.8\n",
    "\n",
    "ece=torch.abs(y_true-y_conf)\n",
    "print(-torch.mean(ece))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import evaluate\n",
    "def calculate_rouge(reference, hypothesis):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, hypothesis)\n",
    "    print(scores)\n",
    "    return scores['rougeL'].fmeasure\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "reference = [\"The cat sat on the mat.\"]\n",
    "hypothesis = [\"The cat sat in the rain\"]\n",
    "scores = [calculate_rouge(i,j) for i,j in zip(reference, hypothesis)]\n",
    "print(scores)\n",
    "\n",
    "\n",
    "# rouge=evaluate.load('rouge')\n",
    "\n",
    "# rouge_results=[rouge.compute(predictiopni,j) for i,j in zip(hypothesis,reference)]\n",
    "# print(rouge_results)\n",
    "# print(rouge_results['rougeL'])\n",
    "# print((rouge_results['rougeL'] > 0.3).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc,roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "def Getfig(fpr,tpr,auc):\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    # plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    # plt.savefig(f\"picture/{data_name}_roc_curve.png\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def aur_trail(activate_time,shuffle):\n",
    "    isshuffle_str=\"shuffle\" if shuffle else \"No_shuffle\"\n",
    "    datapaht=f\"./PACE/response_result/Evaluate_Result_{activate_time}_{isshuffle_str}.json\"\n",
    "    with open(datapaht,'r') as f:\n",
    "        data=json.load(f)\n",
    "    simi_models=[\"Cos_sim\"]\n",
    "    datasets=[\"triviaQA\"]\n",
    "    api_model='gpt-3.5-turbo-0125'\n",
    "    acc_model='bool_acc'\n",
    "    stretagy=[\"vanilla\",'cot',\"multi_step\"]\n",
    "\n",
    "    for sim in simi_models:\n",
    "        for dataset in datasets:\n",
    "            for dd in data:\n",
    "                for k in stretagy:\n",
    "                    # conf_list,Final_conf_list,simi_list,acc_list=[],[],[],[]\n",
    "                    if dd['dataset']==dataset and dd['sim_model']==sim and dd['Stratagy']==k and dd['acc_model']==acc_model and dd['api_model']==api_model:\n",
    "                        print(\"*\"*100)\n",
    "                        print(f\"Load Sucess {dataset} {k} {sim} {acc_model}\")\n",
    "                        acc=np.array(dd['Accuracy'])\n",
    "                        accurate=np.where(acc < 0.55,0,1)\n",
    "                        score1=np.array(dd['Conf'])\n",
    "                        score2=np.array(dd['Pace_Conf'])\n",
    "                        fpr1, tpr1, thresholds1 = roc_curve(accurate, score1)\n",
    "                        fpr2, tpr2, thresholds2 = roc_curve(accurate, score2)\n",
    "                        auc1=auc(fpr1, tpr1)\n",
    "                        auc2=auc(fpr2, tpr2)\n",
    "                        print(np.mean(accurate))\n",
    "                        print(np.mean(score2))\n",
    "                        # print(score2)\n",
    "                        print(auc1,auc2)\n",
    "                        Getfig(fpr1,tpr1,auc1)\n",
    "                        Getfig(fpr2,tpr2,auc2)\n",
    "\n",
    "aur_trail(activate_time=\"20240601\",shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET PACE Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def Print_data(activate_time,shuffle):\n",
    "    isshuffle_str=\"shuffle\" if shuffle else \"No_shuffle\"\n",
    "    datapaht=f\"./PACE/response_result/Evaluate_Result_{activate_time}_{isshuffle_str}.json\"\n",
    "    with open(datapaht,'r') as f:\n",
    "        data=json.load(f)\n",
    "    simi_models=[\"Cos_sim\"]\n",
    "    datasets=[\"triviaQA\"]\n",
    "    api_model='claude-3-5-sonnet-20240620'\n",
    "    acc_model='bool_acc'\n",
    "    stretagy=[\"vanilla\",'cot',\"multi_step\"]\n",
    "\n",
    "    for sim in simi_models:\n",
    "        for dataset in datasets:\n",
    "            for dd in data:\n",
    "                for k in stretagy:\n",
    "                    # conf_list,Final_conf_list,simi_list,acc_list=[],[],[],[]\n",
    "                    if dd['dataset']==dataset and dd['sim_model']==sim and dd['Stratagy']==k and dd['acc_model']==acc_model and dd['api_model']==api_model:\n",
    "                        print(\"*\"*100)\n",
    "                        print(f\"Load Sucess {dataset} {k} {sim} {acc_model}\")\n",
    "                        print(f\"Accuracy {np.mean(np.array(dd['Accuracy']))}\")\n",
    "                        print(f\"ECE {np.mean(np.array(dd['ece']))}\")\n",
    "                        print(f\"ECE after {np.mean(np.array(dd['ece_pace']))}\")\n",
    "                        print(f\"AUROC {np.mean(np.array(dd['auroc']))}\")\n",
    "                        print(f\"AUROC after {np.mean(np.array(dd['auroc_pace']))}\")\n",
    "                        # print(\"*\"*100)\n",
    "Print_data(activate_time=\"20240601\",shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json,random,os\n",
    "\n",
    "# datapath=\"response_result/20240517/din0s_asqa_gpt-3.5-turbo-0125_cot_Long_QA_gpt-3.5-turbo-0125_bertscore.json\"\n",
    "\n",
    "def get_datapath(dataset,api_model,activate_time,acc_model,sim_model,prompt_strategy,isshuffle_str)-> list:\n",
    "    task=[\"QA\",\"Long_QA\"]\n",
    "    datapath=[]\n",
    "    for i in prompt_strategy:\n",
    "        for t in task:\n",
    "            path=f\"response_result/{activate_time}/{dataset}_{api_model}_{i}_{t}_{sim_model}_{acc_model}_{isshuffle_str}.json\"\n",
    "            print(path)\n",
    "            if os.path.isfile(path):\n",
    "                datapath.append(path)\n",
    "\n",
    "    return datapath\n",
    "\n",
    "def mean(data):\n",
    "    return sum(data)/len(data)\n",
    "\n",
    "def Load_data(datapath):\n",
    "    with open(datapath,'r') as f:\n",
    "        data=json.load(f)\n",
    "\n",
    "    conf=[i['Confidence'] for i in data]\n",
    "    ## MAX\n",
    "    simi=[max(map(float,i['Doc_Ans_simi'])) for i in data]\n",
    "    ## Mean\n",
    "    mean_simi=[sum(map(float,i['Doc_Ans_simi']))/len(i['Doc_Ans_simi']) for i in data]\n",
    "    acc=[i['Accuracy'] for i in data]\n",
    "    assert len(simi)==len(conf)\n",
    "    return [conf,mean_simi,acc]\n",
    "\n",
    "# color=['lightblue','lightred',\"lightgreen\",\"yellow\",'pink','lightbrown']\n",
    "def Get_histogram(datalist,dataset,title,stretagy):\n",
    "# Generate sample data\n",
    "    # data1 = np.random.normal(0, 1, 1000)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    colors=plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    # assert len(lable)==len(datalist)\n",
    "    # Plot histograms\n",
    "    for idx,i in enumerate(datalist):\n",
    "        plt.hist(i, bins=100, alpha=0.7, label=stretagy[idx],color=colors[idx% len(colors)])\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f\"{dataset}_{title}\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlim([0, 1])\n",
    "    # plt.ylim([0, 600])\n",
    "    # Show the plot\n",
    "    plt.savefig(f\"picture/histogram/{dataset}_{title}.png\")\n",
    "    # plt.show()\n",
    "    plt.clfPrint_data\n",
    "def show_histogram_graph(vector,title,File_name,stretagy=\"\",sim=\"\",datafile_name=\"\",label=[]):\n",
    "    os.makedirs(f\"PACE/picture/histogram/{File_name}\",exist_ok=True)\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    colors=plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    random.shuffle(colors)\n",
    "    for i,j in zip(vector,label):\n",
    "        plt.hist(i, bins=100, density=True, alpha=0.7, color=colors[random.randint(0,len(colors)-1)], edgecolor='black',label=j)\n",
    "    # Add a title and labels\n",
    "    plt.title(f'{title}')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    # plt.ylim(0,1)\n",
    "    plt.xlim(0,1)\n",
    "    # Add a grid\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='upper right')\n",
    "    # Show plot\n",
    "    plt.savefig(f\"PACE/picture/histogram/{File_name}/{datafile_name}.png\")\n",
    "    # plt.show()\n",
    "    plt.clf\n",
    "\n",
    "def show_plot_graph(vector,title,File_name,stretagy=\"\",sim=\"\",datafile_name=\"\",label=\"\"):\n",
    "    os.makedirs(f\"PACE/picture/histogram/{File_name}\",exist_ok=True)\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(8, 5),facecolor='none')\n",
    "    colors=plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    random.shuffle(colors)\n",
    "\n",
    "    counts, bin_edges = np.histogram(vector, bins=len(vector)//5)\n",
    "    # plt.gca().set_facecolor('lightgrey')\n",
    "    # 計算每個柱的中心點\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    # plt.hist(vector, bins=30, alpha=0.3, color='gray', edgecolor='black')\n",
    "    plt.plot(bin_centers,counts, marker='',linestyle='-', color=colors[random.randint(0,len(colors)-1)],label=label,linewidth=3)\n",
    "    # Add a grid\n",
    "    # plt.grid(True)\n",
    "    plt.xlim(0,1)\n",
    "    plt.legend(loc='upper right', fontsize=20)\n",
    "    # Show plot\n",
    "    plt.savefig(f\"PACE/picture/histogram/{File_name}/{datafile_name}.png\")\n",
    "    # plt.show()\n",
    "    plt.clf\n",
    "\n",
    "def show_plot_graph(vector,title,File_name,stretagy=\"\",sim=\"\",datafile_name=\"\",label=\"\"):\n",
    "    os.makedirs(f\"PACE/picture/histogram/{File_name}\",exist_ok=True)\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(6, 3),facecolor='none')\n",
    "    colors=plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    random.shuffle(colors)\n",
    "\n",
    "    counts, bin_edges = np.histogram(vector, bins=len(vector)//5)\n",
    "    plt.gca().set_facecolor(None)\n",
    "    # 計算每個柱的中心點\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    # plt.hist(vector, bins=30, alpha=0.3, color='gray', edgecolor='black')\n",
    "    plt.plot(bin_centers,counts, marker='',linestyle='-', color=colors[random.randint(0,len(colors)-1)],label=label,linewidth=3)\n",
    "    # Add a grid\n",
    "    # plt.grid(True)\n",
    "    plt.xlim(0,1)\n",
    "    plt.legend(loc='upper left', fontsize=10)\n",
    "    # Show plot\n",
    "    plt.savefig(f\"PACE/picture/histogram/{File_name}/{datafile_name}.png\",transparent=True)\n",
    "    # plt.show()\n",
    "    plt.clf\n",
    "\n",
    "\n",
    "def Update_Fig(activate_time,shuffle):\n",
    "    isshuffle_str=\"shuffle\" if shuffle else \"No_shuffle\"\n",
    "    datapaht=f\"./PACE/response_result/Evaluate_Result_{activate_time}_{isshuffle_str}.json\"\n",
    "    with open(datapaht,'r') as f:\n",
    "        data=json.load(f)\n",
    "\n",
    "    os.makedirs(\"PACE/picture/histogram\",exist_ok=True)\n",
    "    simi_models=[\"Cos_sim\"]\n",
    "    datasets=[\"din0s/asqa\"]\n",
    "    api_model='gpt-3.5-turbo-0125'\n",
    "    acc_model='rougeL'\n",
    "\n",
    "    stretagy=[\"vanilla\",'cot',\"multi_step\"]\n",
    "\n",
    "    for sim in simi_models:\n",
    "        for dataset in datasets:\n",
    "            for dd in data:\n",
    "                for k in stretagy:\n",
    "                    # conf_list,Final_conf_list,simi_list,acc_list=[],[],[],[]\n",
    "                    if dd['dataset']==dataset and dd['sim_model']==sim and dd['Stratagy']==k and dd['acc_model']==acc_model:\n",
    "                        print(f\"Load Sucess {dataset} {k} {sim}\")\n",
    "                        print(f\"Accuracy {np.mean(np.array(dd['Accuracy']))}\")\n",
    "                        print(f\"ECE {np.mean(np.array(dd['ece']))}\")\n",
    "                        print(f\"AUROC {np.mean(np.array(dd['auroc']))}\")\n",
    "                        dataset_path=dataset.replace(\"/\",\"_\")\n",
    "                        print(dd['Simi'])\n",
    "                        show_plot_graph(dd['Conf'],File_name=f\"{activate_time}_{isshuffle_str}\",title=f\"{k} {sim} Confidence\",stretagy=k,sim=f\"{sim}\",datafile_name=f\"{dataset_path}_{isshuffle_str}_{sim}_{k}_Confidence\",label=f\"Confidence\")\n",
    "\n",
    "                        show_plot_graph(dd['Simi'],File_name=f\"{activate_time}_{isshuffle_str}\",title=f\"{k} {sim} Similarity\",stretagy=k,sim=f\"{sim}\",datafile_name=f\"{dataset_path}_{isshuffle_str}_{sim}_{k}_Similarity\",label=f\"Similarity\")\n",
    "\n",
    "                        show_plot_graph(dd['Pace_Conf'],File_name=f\"{activate_time}_{isshuffle_str}\",title=f\"{k} {sim} Final Confidence\",stretagy=k,sim=f\"{sim}\",datafile_name=f\"{dataset_path}_{isshuffle_str}_{sim}_{k}_PACE_Confidence\",label=f\"PACE Confidence\")\n",
    "\n",
    "                        show_plot_graph(dd['Accuracy'],File_name=f\"{activate_time}_{isshuffle_str}\",title=f\"{k} {sim} Accuracy\",stretagy=k,sim=f\"{sim}\",datafile_name=f\"{dataset_path}_{isshuffle_str}_{sim}_{k}_Accuracy\",label=f\"Accuracy\")\n",
    "\n",
    "Update_Fig(\"20240601\",False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json,random,os\n",
    "\n",
    "def show_four_plot(vectorlist,title):\n",
    "    colors=plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    random.shuffle(colors)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    fig.patch.set_facecolor('lightgrey')\n",
    "    # Plot each vector list in its respective subplot\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        counts, bin_edges = np.histogram(vectorlist[i], bins=len(vectorlist[i])//10)\n",
    "        # 計算每個柱的中心點\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        # plt.hist(vector, bins=30, alpha=0.3, color='gray', edgecolor='black')\n",
    "\n",
    "        ax.plot(bin_centers,counts, marker='.',linestyle='-', color=colors[random.randint(0,len(colors)-1)],label='No Shuffle')\n",
    "        # ax.set_xlabel('Index')\n",
    "        if i%2:\n",
    "            ax.set_ylabel('Accuracy',fontsize=12)\n",
    "        else:\n",
    "            ax.set_ylabel('Confidence',fontsize=12)\n",
    "        ax.set_xlim((0,1))\n",
    "        ax.set_facecolor('whitesmoke')\n",
    "    # fig.suptitle(title, fontsize=16)\n",
    "    # Adjust layout to prevent overlapping\n",
    "    plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def Update_Fig(activate_time,shuffle):\n",
    "    isshuffle_str=\"shuffle\" if shuffle else \"No_shuffle\"\n",
    "    datapaht=f\"./PACE/response_result/Evaluate_Result_{activate_time}_{isshuffle_str}.json\"\n",
    "    with open(datapaht,'r') as f:\n",
    "        data=json.load(f)\n",
    "\n",
    "    os.makedirs(\"PACE/picture/histogram\",exist_ok=True)\n",
    "    simi_models=[\"Cos_sim\"]\n",
    "    datasets=[\"din0s/asqa\"]\n",
    "    api_model='gpt-3.5-turbo-0125'\n",
    "    acc_model='rougeL'\n",
    "\n",
    "    stretagy=[\"vanilla\",'cot',\"multi_step\"]\n",
    "    vector=[]\n",
    "    for sim in simi_models:\n",
    "        for dataset in datasets:\n",
    "            for dd in data:\n",
    "                for k in stretagy:\n",
    "                    # conf_list,Final_conf_list,simi_list,acc_list=[],[],[],[]\n",
    "                    if dd['dataset']==dataset and dd['sim_model']==sim and dd['Stratagy']==k and dd['acc_model']==acc_model:\n",
    "                        vector+=[dd['Conf'],dd['Accuracy']]\n",
    "                        show_four_plot([dd['Conf'],dd['Accuracy']],k)\n",
    "\n",
    "Update_Fig(20240601,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_histogram_graph(vector,dim_x_y,stretagy=\"\"):\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.hist(vector, bins=30, density=True, alpha=0.7, color='blue', edgecolor='black')\n",
    "    # Add a title and labels\n",
    "    plt.title(f'{stretagy} Score')\n",
    "    plt.xlabel('Confidence Value')\n",
    "    plt.ylabel('Density')\n",
    "    # plt.ylim(0,1)\n",
    "    plt.xlim(0,1)\n",
    "    # Add a grid\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show plot\n",
    "    # plt.savefig(f\"picture/histogram/{dataset}_{title}.png\")\n",
    "    plt.show()\n",
    "    plt.clf\n",
    "\n",
    "from netcal.metrics import ECE\n",
    "\n",
    "activate_time=\"20240529\"\n",
    "\n",
    "os.makedirs(\"picture/histogram\",exist_ok=True)\n",
    "simi_models=[\"Cos_sim\",\"snli\"]\n",
    "datasets=[\"natural_questions\",'din0s_asqa']\n",
    "api_model='gpt-3.5-turbo-0125'\n",
    "acc_model='bertscore'\n",
    "\n",
    "stretagy=[\"vanilla\",'cot',\"multi_step\"]\n",
    "\n",
    "simi_model=\"snli\"\n",
    "dataset=\"din0s_asqa\"\n",
    "\n",
    "path_list=get_datapath(dataset=dataset,api_model=api_model,activate_time=activate_time,acc_model=acc_model,sim_model=simi_model,prompt_strategy=stretagy)\n",
    "\n",
    "evaldata=list(map(Load_data,path_list))\n",
    "\n",
    "for k in range(3):\n",
    "    accuracy=np.mean(np.array(evaldata[k][2]))\n",
    "    y_confs=evaldata[k][0]\n",
    "    y_true=list(map(float,evaldata[k][2]))\n",
    "    y_true=np.where(np.array(y_true) < 0.9,0,1)\n",
    "\n",
    "    ll=0.5\n",
    "    pace_conf_array = np.add(ll*np.array(evaldata[k][0]),(1-ll)*np.array(evaldata[k][1]))\n",
    "\n",
    "    # ECE\n",
    "    n_bins = 10\n",
    "    # diagram = ReliabilityDiagram(n_bins)\n",
    "    ece = ECE(n_bins)\n",
    "    assert len(y_confs)==len(y_true)\n",
    "    ece_score = ece.measure(np.array(y_confs), np.array(y_true),uncertainty='mean')\n",
    "    print(\"ECE:\", ece_score)\n",
    "\n",
    "    n_bins = 10\n",
    "    # diagram = ReliabilityDiagram(n_bins)\n",
    "    ece = ECE(n_bins)\n",
    "    ece_pace_score = ece.measure(pace_conf_array, np.array(y_true))\n",
    "    print(\"ECE_PACE:\", ece_pace_score)\n",
    "\n",
    "    print(f\"conf origin {np.mean(y_confs)}, PACE {np.mean(pace_conf_array)}\")\n",
    "\n",
    "    show_histogram_graph(y_confs,dim_x_y=[0,1],stretagy=f\"{simi_model}\")\n",
    "\n",
    "    show_histogram_graph(evaldata[k][1],dim_x_y=[0,1],stretagy=f\"{simi_model}\")\n",
    "\n",
    "    show_histogram_graph(pace_conf_array,dim_x_y=[0,1],stretagy=f\"{simi_model}\")\n",
    "\n",
    "    # show_histogram_graph(y_true,dim_x_y=[0,1],stretagy=f\"{simi_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "def split_text_into_fixed_length_parts(text, tokens_per_part, model_name='bert'):\n",
    "\n",
    "    model_huggingface={\n",
    "        'bert':'bert-base-uncased',\n",
    "        'xbert':'efederici/sentence-bert-base',\n",
    "    }\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_huggingface[model_name])\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Initialize the list to hold each part\n",
    "    parts = []\n",
    "\n",
    "    # Calculate the number of full parts\n",
    "    full_parts = len(tokens) // tokens_per_part\n",
    "\n",
    "    # Create parts with exactly tokens_per_part tokens\n",
    "    for i in range(full_parts):\n",
    "        start_index = i * tokens_per_part\n",
    "        end_index = start_index + tokens_per_part\n",
    "        part_tokens = tokens[start_index:end_index]\n",
    "        # Convert token list to string and add to the parts list\n",
    "        parts.append(tokenizer.convert_tokens_to_string(part_tokens))\n",
    "\n",
    "    # Handle the remaining tokens, if any\n",
    "    if len(tokens) % tokens_per_part:\n",
    "        remaining_tokens = tokens[full_parts * tokens_per_part:]\n",
    "        parts.append(tokenizer.convert_tokens_to_string(remaining_tokens))\n",
    "\n",
    "    return parts\n",
    "\n",
    "# Example usage\n",
    "text = \"Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.Here is an example text that we want to split into parts where each part has exactly 96 tokens, using a tokenizer from Hugging Face.\"\n",
    "tokens_per_part = 96\n",
    "result = split_text_into_fixed_length_parts(text, tokens_per_part,'bert')\n",
    "result = split_text_into_fixed_length_parts(text, tokens_per_part,'xbert')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "from evaluate import load\n",
    "def load_data():\n",
    "    with open('/home/thesis/confidence_Score/response_result/gpt-3.5-turbo-0125_vanilla_simi_2024_05_08.json','r') as f:\n",
    "        data=json.load(f)\n",
    "    # print(data.keys())\n",
    "    simi_res=data['When did the kokoda war start and end?']['similarity_res']\n",
    "    olddat=simi_res[0]\n",
    "    for i in simi_res:\n",
    "        if i[1] > olddat[1]:\n",
    "            olddat=i\n",
    "    ans,long_ans=olddat[2],olddat[3]\n",
    "    return ans,long_ans\n",
    "def Bernormalize(ans,long_ans):\n",
    "    nomalizer=BertNormalizer(clean_text=True,lowercase=True,handle_chinese_chars=True)\n",
    "    return nomalizer.normalize_str(ans),nomalizer.normalize_str(long_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WER metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans,long_ans=load_data()\n",
    "print(ans)\n",
    "print(long_ans)\n",
    "wer_metric = load(\"wer\")\n",
    "acc_wer = wer_metric.compute(references=[ans], predictions=[long_ans])\n",
    "print(f\"WER acc : {acc_wer}\")\n",
    "print(f\"1- WER acc : {1-acc_wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans,long_ans=load_data()\n",
    "print(ans)\n",
    "print(long_ans)\n",
    "exact_match_metric = load(\"exact_match\")\n",
    "results = exact_match_metric.compute(predictions=[ans], references=[long_ans])\n",
    "print(results['exact_match'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "ans,long_ans=load_data()\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embedding_1= model.encode(ans, convert_to_tensor=True)\n",
    "embedding_2 = model.encode(long_ans, convert_to_tensor=True)\n",
    "\n",
    "result=util.pytorch_cos_sim(embedding_1, embedding_2)\n",
    "\n",
    "print(result.item())\n",
    "## tensor([[0.6003]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "from evaluate import load\n",
    "ans,long_ans=load_data()\n",
    "print(ans)\n",
    "print(long_ans)\n",
    "\n",
    "\n",
    "P, R, F1 = score([ans], [long_ans],lang=\"en\",verbose=True)\n",
    "# Print scores\n",
    "print(\"Precision: \", P.item())\n",
    "print(\"Recall: \", R.item())\n",
    "print(\"F1 Score: \", F1.item())\n",
    "\n",
    "\n",
    "bertscore = load(\"bertscore\")\n",
    "result=bertscore.compute(predictions=[ans],references=[long_ans],lang='en',verbose=True)\n",
    "print(\"Precision: \", result['precision'].pop())\n",
    "print(\"Recall: \", result['recall'].pop())\n",
    "print(\"F1 Score: \", result['f1'].pop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import load_checkpoint\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "datapath=f'response_result/gpt-3.5-turbo-0125_vanilla_2024_05_09.json'\n",
    "datadict=load_checkpoint(datapath)\n",
    "\n",
    "class eval_dataloader:\n",
    "    def __init__(self,dataset_path,batch_size=1,purpose='eval') -> None:\n",
    "        self.dataset = load_checkpoint(dataset_path)\n",
    "        if purpose==\"eval\":\n",
    "            self.loader=DataLoader(list(self.dataset.values()),batch_size=batch_size,collate_fn=self.simi_acc_collate_fn,shuffle=True)\n",
    "        elif purpose==\"ece\":\n",
    "            self.loader=DataLoader(list(self.dataset.values()),batch_size=batch_size,collate_fn=self.ece_collate_fn,shuffle=False)\n",
    "\n",
    "    def simi_acc_collate_fn(self,batch):\n",
    "        res=[]\n",
    "        for i in batch:\n",
    "            res.append([i['Question'],i['Document'],i['Answer'],i['Long Answer'],i['Confidence']])\n",
    "        return res\n",
    "\n",
    "    def ece_collate_fn(self,batch):\n",
    "        simi_res=[]\n",
    "        conf_res=[]\n",
    "        accres=[]\n",
    "        for i in batch:\n",
    "            simi_res.append(i['similarity_res'])\n",
    "            conf_res.append(i['confidence'])\n",
    "            accres.append(i['acc'])\n",
    "        return simi_res,conf_res,accres\n",
    "\n",
    "\n",
    "def conf_calibration(simi,conf):\n",
    "    x_lambda=0.5\n",
    "    return x_lambda*simi+(1-x_lambda)*conf\n",
    "\n",
    "def ece_calibration(simi:list,acc:list,conf:list): # batch b_m\n",
    "    conf=list(map(conf_calibration,simi,conf))\n",
    "    assert len(acc)==len(simi)\n",
    "    b_m=len(acc)\n",
    "    ece=np.mean(np.array(acc)-np.array(conf))/b_m\n",
    "    return ece\n",
    "\n",
    "def get_most_high_simi(simi_res:list):\n",
    "    return sorted(simi_res,key=lambda x:x[1],reverse=True)[0][1]\n",
    "\n",
    "eve_l=eval_dataloader(datapath,2,'eval').loader\n",
    "print(len(eve_l))\n",
    "# for simi_res,conf_res,accres in eve_l:\n",
    "    # most_simi=list(map(get_most_high_simi,simi_res))\n",
    "    # print(ece_calibration(most_simi,accres,conf_res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "\n",
    "def compute_macro_f1(sentence1, sentence2):\n",
    "    # Tokenize the sentences\n",
    "    tokens1 = sentence1.split()\n",
    "    tokens2 = sentence2.split()\n",
    "\n",
    "    # Count the frequency of each token in both sentences\n",
    "    counter1 = Counter(tokens1)\n",
    "    counter2 = Counter(tokens2)\n",
    "\n",
    "    # Create a union of all unique tokens\n",
    "    all_tokens = list(set(tokens1) | set(tokens2))\n",
    "\n",
    "    # Create binary vectors for comparison\n",
    "    vector1 = torch.tensor([counter1[token] for token in all_tokens])\n",
    "    vector2 = torch.tensor([counter2[token] for token in all_tokens])\n",
    "\n",
    "    # Compute the macro-F1 score\n",
    "    f1 = torch.tensor(f1_score(vector1.numpy(), vector2.numpy(), average='macro'),dtype=torch.float16)\n",
    "\n",
    "    return f1\n",
    "\n",
    "# Example sentences\n",
    "sentence1 = \"The quick brown fox jumps over the lazy dog\"\n",
    "sentence2 = \"A quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "macro_f1 = compute_macro_f1(sentence1, sentence2)\n",
    "print(\"Macro F1 Score:\", macro_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Money Spent on API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob\n",
    "from util import load_checkpoint\n",
    "def Calulate_spent(file_path='response_result/gpt-3.5-turbo-0125',api_model=\"gpt-3.5-turbo-0125\"):\n",
    "    compete_toekn=0\n",
    "    prompt_token=0\n",
    "\n",
    "    for datapath in glob.glob(f'{file_path}*.json'):\n",
    "        if os.path.isfile(datapath):\n",
    "            datares=load_checkpoint(datapath)\n",
    "            print(f\"{datapath} {len(datares)}\")\n",
    "            for i in datares.values():\n",
    "                if \"Complete_tokens\" in i:\n",
    "                    compete_toekn+=i['Complete_tokens']\n",
    "                if \"Prompt_tokens\" in i:\n",
    "                    prompt_token+=i['Prompt_tokens']\n",
    "\n",
    "    Total_Spent =compete_toekn*8.00/1000000+prompt_token*6.00/1000000\n",
    "    print(f\"Complete tokens :{compete_toekn},prompt_tokens :{prompt_token} \")\n",
    "    print(f\"Total Spent {Total_Spent} USD ; {Total_Spent*30} TWD\")\n",
    "\n",
    "\n",
    "Calulate_spent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "def is_answer_correct(question, true_answer, predicted_answer):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Get embeddings\n",
    "    true_embedding = get_embedding(true_answer, model, tokenizer)\n",
    "    predicted_embedding = get_embedding(predicted_answer, model, tokenizer)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(true_embedding.numpy(), predicted_embedding.numpy())[0][0]\n",
    "\n",
    "    # Threshold for correctness\n",
    "    threshold = 0.8\n",
    "    print(similarity)\n",
    "    return similarity >= threshold\n",
    "\n",
    "# Example question and answers\n",
    "question = \"What is the capital of France?\"\n",
    "true_answer = \"The capital of France is Paris.\"\n",
    "predicted_answer = \"Paris is the capital of France.\"\n",
    "\n",
    "is_correct = is_answer_correct(question, true_answer, predicted_answer)\n",
    "print(\"Is the answer correct?\", is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    # Simple sentence tokenizer (could be improved with more sophisticated methods)\n",
    "    return text.split('. ')\n",
    "\n",
    "def evaluate_long_form_qa(true_answer, predicted_answer):\n",
    "    # Tokenize the answers into sentences\n",
    "    true_sentences = tokenize_sentences(true_answer)\n",
    "    predicted_sentences = tokenize_sentences(predicted_answer)\n",
    "\n",
    "    # Generate binary labels\n",
    "    true_labels = [1] * len(true_sentences)  # Assume all sentences in the ground truth are correct\n",
    "    predicted_labels = [1 if sentence in true_sentences else 0 for sentence in predicted_sentences]\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Example long-form answers\n",
    "true_answer = \"The quick brown fox jumps over the lazy dog. Foxes are known for their agility. Dogs can be quite lazy at times.\"\n",
    "predicted_answer = \"The quick brown fox jumps over the lazy dog. Foxes are very agile animals. Sometimes dogs are lazy.\"\n",
    "\n",
    "precision, recall, f1 = evaluate_long_form_qa(true_answer, predicted_answer)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "eval_model = BERTScorer(\n",
    "                        model_type=\"bert-base-uncased\",  # Model type\n",
    "                        num_layers=9,  # Number of layers to use\n",
    "                        all_layers=False,  # Whether to use all layers\n",
    "                        idf=False,  # Whether to use IDF scaling\n",
    "                        batch_size=64,  # Batch size\n",
    "                        lang=None,  # Language of the texts, auto-detect based on model if None\n",
    "                        rescale_with_baseline=False,  # Whether to rescale\n",
    "                        device='cuda:1'\n",
    "                    )\n",
    "def tokenize_sentences(text):\n",
    "    # Simple sentence tokenizer (could be improved with more sophisticated methods)\n",
    "    return text.split('. ')\n",
    "true_answer = \"The quick brown fox jumps over the lazy dog. Foxes are known for their agility. Dogs can be quite lazy at times.\"\n",
    "predicted_answer = \"The quick brown fox. The quick brown fox. The quick brown fox.\"\n",
    "\n",
    "true_sentences=tokenize_sentences(true_answer)\n",
    "predicted_sentences=tokenize_sentences(predicted_answer)\n",
    "true_labels = [1] * len(true_sentences)  # Assume all sentences in the ground truth are correct\n",
    "predicted_labels = [1 if sentence in true_sentences else 0 for sentence in predicted_sentences]\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "conf_path='response_result/gpt-3.5-turbo-0125_vanilla_2024_05_09.json'\n",
    "simi_acc_path='response_result/gpt-3.5-turbo-0125_vanilla_simi_acc_2024_05_09.json'\n",
    "with open(simi_acc_path,'r') as f:\n",
    "    data1=json.load(f)\n",
    "    print(len(data1))\n",
    "    for i in data1.keys():\n",
    "        print(i)\n",
    "\n",
    "with open(conf_path,'r') as f:\n",
    "    data2=json.load(f)\n",
    "    print(len(data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def show(conf_list):\n",
    "    if isinstance(conf_list,dict):\n",
    "        print(f\"Evaluate Result:\")\n",
    "        for v in conf_list.values():\n",
    "            print(f\"      {v['Stratagy']}:\")\n",
    "            for k1,v1 in v.items():\n",
    "                print(f\"        {k1} : {v1}\")\n",
    "    elif isinstance(conf_list,list):\n",
    "        print(f\"Evaluate Result:\")\n",
    "        for v in conf_list:\n",
    "            print(f\"      {v['Stratagy']}:\")\n",
    "            for k1,v1 in v.items():\n",
    "                print(f\"        {k1} : {v1}\")\n",
    "\n",
    "for i in glob.glob(\"response_result/*_eval_*.json\"):\n",
    "    with open(i,'r') as f:\n",
    "        show(json.load(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a =[torch.tensor(i/100) for i in range(0,100)]\n",
    "print(a)\n",
    "print(torch.stack(a,dim=0))\n",
    "# print(torch.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "confidence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
